@inproceedings{hayes-2017-improving-robot-controller,
  title = {Improving {{Robot Controller Transparency Through Autonomous Policy Explanation}}},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  author = {Hayes, Bradley and Shah, Julie A.},
  date = {2017-03-06},
  pages = {303--312},
  publisher = {ACM},
  location = {Vienna Austria},
  doi = {10.1145/2909824.3020233},
  url = {https://dl.acm.org/doi/10.1145/2909824.3020233},
  urldate = {2023-03-06},
  eventtitle = {{{HRI}} '17: {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  isbn = {978-1-4503-4336-7},
  langid = {english},
  file = {/Users/erugfra/Library/CloudStorage/GoogleDrive-franco.ruggeri.pro@gmail.com/My Drive/3-resources/literature/hayes-2017.pdf;/Users/erugfra/Zotero/storage/QLNQLUR7/hayes-2017.pdf}
}

@book{kastner-2025-machine-learning-production,
  title = {Machine Learning in Production: From Models to Products},
  shorttitle = {Machine Learning in Production},
  author = {Kästner, Christian},
  date = {2025},
  edition = {First edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  abstract = {"This book covers how to build software products with machine-learning components and provides a holistic view of ML systems built to achieve safety, security, usability, fairness in the real world"--},
  isbn = {978-0-262-04972-6},
  langid = {english},
  pagetotal = {1},
  keywords = {@action,@p/low}
}

@inproceedings{lu-2022-roadmap-software-engineering,
  title = {Towards a Roadmap on Software Engineering for Responsible {{AI}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{AI Engineering}}: {{Software Engineering}} for {{AI}}},
  author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Xing, Zhenchang},
  date = {2022-05-16},
  pages = {101--112},
  publisher = {ACM},
  location = {Pittsburgh Pennsylvania},
  doi = {10.1145/3522664.3528607},
  url = {https://dl.acm.org/doi/10.1145/3522664.3528607},
  urldate = {2025-11-12},
  eventtitle = {{{CAIN}} '22: 1st {{Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  isbn = {978-1-4503-9275-4},
  langid = {english},
  keywords = {@action}
}

@inproceedings{nguyen-2024-novel-contractbased-runtime,
  title = {Novel {{Contract-based Runtime Explainability Framework}} for {{End-to-End Ensemble Machine Learning Serving}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 3rd {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  author = {Nguyen, Minh-Tri and Truong, Hong-Linh and Truong-Huu, Tram},
  date = {2024-04-14},
  pages = {234--244},
  publisher = {ACM},
  location = {Lisbon Portugal},
  doi = {10.1145/3644815.3644964},
  url = {https://dl.acm.org/doi/10.1145/3644815.3644964},
  urldate = {2025-11-12},
  eventtitle = {{{CAIN}} 2024: {{IEEE}}/{{ACM}} 3rd {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  isbn = {979-8-4007-0591-5},
  langid = {english}
}

@article{ribeiro-2018-anchors-highprecision-modelagnostic,
  title = {Anchors: {{High-Precision Model-Agnostic Explanations}}},
  shorttitle = {Anchors},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2018-04-25},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11491},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
  urldate = {2025-11-10},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  keywords = {@action,@p/low}
}

@online{ruggeri-2025-explainable-reinforcement-learninga,
  title = {Explainable {{Reinforcement Learning}} via {{Temporal Policy Decomposition}}},
  author = {Ruggeri, Franco and Russo, Alessio and Inam, Rafia and Johansson, Karl Henrik},
  date = {2025-01-07},
  eprint = {2501.03902},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.03902},
  url = {http://arxiv.org/abs/2501.03902},
  urldate = {2025-01-08},
  abstract = {We investigate the explainability of Reinforcement Learning (RL) policies from a temporal perspective, focusing on the sequence of future outcomes associated with individual actions. In RL, value functions compress information about rewards collected across multiple trajectories and over an infinite horizon, allowing a compact form of knowledge representation. However, this compression obscures the temporal details inherent in sequential decision-making, presenting a key challenge for interpretability. We present Temporal Policy Decomposition (TPD), a novel explainability approach that explains individual RL actions in terms of their Expected Future Outcome (EFO). These explanations decompose generalized value functions into a sequence of EFOs, one for each time step up to a prediction horizon of interest, revealing insights into when specific outcomes are expected to occur. We leverage fixed-horizon temporal difference learning to devise an off-policy method for learning EFOs for both optimal and suboptimal actions, enabling contrastive explanations consisting of EFOs for different state-action pairs. Our experiments demonstrate that TPD generates accurate explanations that (i) clarify the policy's future strategy and anticipated trajectory for a given action and (ii) improve understanding of the reward composition, facilitating fine-tuning of the reward function to align with human expectations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@article{terra-2022-beerl-both-ends,
  title = {{{BEERL}}: {{Both Ends Explanations}} for {{Reinforcement Learning}}},
  shorttitle = {{{BEERL}}},
  author = {Terra, Ahmad and Inam, Rafia and Fersman, Elena},
  date = {2022-10-28},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {12},
  number = {21},
  pages = {10947},
  issn = {2076-3417},
  doi = {10.3390/app122110947},
  url = {https://www.mdpi.com/2076-3417/12/21/10947},
  urldate = {2022-11-21},
  abstract = {Deep Reinforcement Learning (RL) is a black-box method and is hard to understand because the agent employs a neural network (NN). To explain the behavior and decisions made by the agent, different eXplainable RL (XRL) methods are developed; for example, feature importance methods are applied to analyze the contribution of the input side of the model, and reward decomposition methods are applied to explain the components of the output end of the RL model. In this study, we present a novel method to connect explanations from both input and output ends of a black-box model, which results in fine-grained explanations. Our method exposes the reward prioritization to the user, which in turn generates two different levels of explanation and allows RL agent reconfigurations when unwanted behaviors are observed. The method further summarizes the detailed explanations into a focus value that takes into account all reward components and quantifies the fulfillment of the explanation of desired properties. We evaluated our method by applying it to a remote electrical telecom-antenna-tilt use case and two openAI gym environments: lunar lander and cartpole. The results demonstrated fine-grained explanations by detailing input features’ contributions to certain rewards and revealed biases of the reward components, which are then addressed by adjusting the reward’s weights.},
  langid = {english}
}
