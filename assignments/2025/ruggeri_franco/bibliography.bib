@inproceedings{hayes-2017-improving-robot-controller,
  title = {Improving {{Robot Controller Transparency Through Autonomous Policy Explanation}}},
  booktitle = {Proceedings of the 2017 {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  author = {Hayes, Bradley and Shah, Julie A.},
  date = {2017-03-06},
  pages = {303--312},
  publisher = {ACM},
  location = {Vienna Austria},
  doi = {10.1145/2909824.3020233},
  url = {https://dl.acm.org/doi/10.1145/2909824.3020233},
  urldate = {2023-03-06},
  eventtitle = {{{HRI}} '17: {{ACM}}/{{IEEE International Conference}} on {{Human-Robot Interaction}}},
  isbn = {978-1-4503-4336-7},
  langid = {english},
  file = {/Users/erugfra/Library/CloudStorage/GoogleDrive-franco.ruggeri.pro@gmail.com/My Drive/3-resources/literature/hayes-2017.pdf;/Users/erugfra/Zotero/storage/QLNQLUR7/hayes-2017.pdf}
}

@book{kastner-2025-machine-learning-production,
  title = {Machine Learning in Production: From Models to Products},
  shorttitle = {Machine Learning in Production},
  author = {Kästner, Christian},
  date = {2025},
  edition = {First edition},
  publisher = {The MIT Press},
  location = {Cambridge, Massachusetts},
  abstract = {"This book covers how to build software products with machine-learning components and provides a holistic view of ML systems built to achieve safety, security, usability, fairness in the real world"--},
  isbn = {978-0-262-04972-6},
  langid = {english},
  pagetotal = {1},
  keywords = {@action,@p/low}
}

@article{lavin-2022-technology-readiness-levels,
  title = {Technology Readiness Levels for Machine Learning Systems},
  author = {Lavin, Alexander and Gilligan-Lee, Ciarán M. and Visnjic, Alessya and Ganju, Siddha and Newman, Dava and Ganguly, Sujoy and Lange, Danny and Baydin, Atílím Güneş and Sharma, Amit and Gibson, Adam and Zheng, Stephan and Xing, Eric P. and Mattmann, Chris and Parr, James and Gal, Yarin},
  date = {2022-10-20},
  journaltitle = {Nature Communications},
  shortjournal = {Nat Commun},
  volume = {13},
  number = {1},
  pages = {6039},
  issn = {2041-1723},
  doi = {10.1038/s41467-022-33128-9},
  url = {https://www.nature.com/articles/s41467-022-33128-9},
  urldate = {2025-11-10},
  abstract = {Abstract                            The development and deployment of machine learning systems can be executed easily with modern tools, but the process is typically rushed and means-to-an-end. Lack of diligence can lead to technical debt, scope creep and misaligned objectives, model misuse and failures, and expensive consequences. Engineering systems, on the other hand, follow well-defined processes and testing standards to streamline development for high-quality, reliable results. The extreme is spacecraft systems, with mission critical measures and robustness throughout the process. Drawing on experience in both spacecraft engineering and machine learning (research through product across domain areas), we’ve developed a proven systems engineering approach for machine learning and artificial intelligence: the               Machine Learning Technology Readiness Levels               framework defines a principled process to ensure robust, reliable, and responsible systems while being streamlined for machine learning workflows, including key distinctions from traditional software engineering, and a lingua franca for people across teams and organizations to work collaboratively on machine learning and artificial intelligence technologies. Here we describe the framework and elucidate with use-cases from physics research to computer vision apps to medical diagnostics.},
  langid = {english}
}

@inproceedings{lu-2022-roadmap-software-engineering,
  title = {Towards a Roadmap on Software Engineering for Responsible {{AI}}},
  booktitle = {Proceedings of the 1st {{International Conference}} on {{AI Engineering}}: {{Software Engineering}} for {{AI}}},
  author = {Lu, Qinghua and Zhu, Liming and Xu, Xiwei and Whittle, Jon and Xing, Zhenchang},
  date = {2022-05-16},
  pages = {101--112},
  publisher = {ACM},
  location = {Pittsburgh Pennsylvania},
  doi = {10.1145/3522664.3528607},
  url = {https://dl.acm.org/doi/10.1145/3522664.3528607},
  urldate = {2025-11-12},
  eventtitle = {{{CAIN}} '22: 1st {{Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  isbn = {978-1-4503-9275-4},
  langid = {english},
  keywords = {@action}
}

@inproceedings{nguyen-2024-novel-contractbased-runtime,
  title = {Novel {{Contract-based Runtime Explainability Framework}} for {{End-to-End Ensemble Machine Learning Serving}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 3rd {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  author = {Nguyen, Minh-Tri and Truong, Hong-Linh and Truong-Huu, Tram},
  date = {2024-04-14},
  pages = {234--244},
  publisher = {ACM},
  location = {Lisbon Portugal},
  doi = {10.1145/3644815.3644964},
  url = {https://dl.acm.org/doi/10.1145/3644815.3644964},
  urldate = {2025-11-12},
  eventtitle = {{{CAIN}} 2024: {{IEEE}}/{{ACM}} 3rd {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  isbn = {979-8-4007-0591-5},
  langid = {english}
}

@article{ribeiro-2018-anchors-highprecision-modelagnostic,
  title = {Anchors: {{High-Precision Model-Agnostic Explanations}}},
  shorttitle = {Anchors},
  author = {Ribeiro, Marco Tulio and Singh, Sameer and Guestrin, Carlos},
  date = {2018-04-25},
  journaltitle = {Proceedings of the AAAI Conference on Artificial Intelligence},
  shortjournal = {AAAI},
  volume = {32},
  number = {1},
  issn = {2374-3468, 2159-5399},
  doi = {10.1609/aaai.v32i1.11491},
  url = {https://ojs.aaai.org/index.php/AAAI/article/view/11491},
  urldate = {2025-11-10},
  abstract = {We introduce a novel model-agnostic system that explains the behavior of complex models with high-precision rules called anchors, representing local, "sufficient" conditions for predictions. We propose an algorithm to efficiently compute these explanations for any black-box model with high-probability guarantees. We demonstrate the flexibility of anchors by explaining a myriad of different models for different domains and tasks. In a user study, we show that anchors enable users to predict how a model would behave on unseen instances with less effort and higher precision, as compared to existing linear explanations or no explanations.},
  keywords = {@action,@p/low}
}

@book{ruggeri-2025-explainable-reinforcement-learning,
  title = {Explainable {{Reinforcement Learning}} for {{Mobile Network Optimization}}},
  author = {Ruggeri, Franco},
  date = {2025},
  publisher = {KTH Royal Institute of Technology},
  location = {Stockholm, Sweden},
  isbn = {978-91-8106-180-2},
  pagetotal = {113}
}

@online{ruggeri-2025-explainable-reinforcement-learninga,
  title = {Explainable {{Reinforcement Learning}} via {{Temporal Policy Decomposition}}},
  author = {Ruggeri, Franco and Russo, Alessio and Inam, Rafia and Johansson, Karl Henrik},
  date = {2025-01-07},
  eprint = {2501.03902},
  eprinttype = {arXiv},
  eprintclass = {cs},
  doi = {10.48550/arXiv.2501.03902},
  url = {http://arxiv.org/abs/2501.03902},
  urldate = {2025-01-08},
  abstract = {We investigate the explainability of Reinforcement Learning (RL) policies from a temporal perspective, focusing on the sequence of future outcomes associated with individual actions. In RL, value functions compress information about rewards collected across multiple trajectories and over an infinite horizon, allowing a compact form of knowledge representation. However, this compression obscures the temporal details inherent in sequential decision-making, presenting a key challenge for interpretability. We present Temporal Policy Decomposition (TPD), a novel explainability approach that explains individual RL actions in terms of their Expected Future Outcome (EFO). These explanations decompose generalized value functions into a sequence of EFOs, one for each time step up to a prediction horizon of interest, revealing insights into when specific outcomes are expected to occur. We leverage fixed-horizon temporal difference learning to devise an off-policy method for learning EFOs for both optimal and suboptimal actions, enabling contrastive explanations consisting of EFOs for different state-action pairs. Our experiments demonstrate that TPD generates accurate explanations that (i) clarify the policy's future strategy and anticipated trajectory for a given action and (ii) improve understanding of the reward composition, facilitating fine-tuning of the reward function to align with human expectations.},
  pubstate = {prepublished},
  keywords = {Computer Science - Artificial Intelligence,Computer Science - Machine Learning}
}

@inproceedings{terra-2020-explainability-methods-identifying,
  title = {Explainability {{Methods}} for {{Identifying Root-Cause}} of {{SLA Violation Prediction}} in {{5G Network}}},
  booktitle = {{{GLOBECOM}} 2020 - 2020 {{IEEE Global Communications Conference}}},
  author = {Terra, Ahmad and Inam, Rafia and Baskaran, Sandhya and Batista, Pedro and Burdick, Ian and Fersman, Elena},
  date = {2020-12},
  pages = {1--7},
  publisher = {IEEE},
  location = {Taipei, Taiwan},
  doi = {10.1109/GLOBECOM42002.2020.9322496},
  url = {https://ieeexplore.ieee.org/document/9322496/},
  urldate = {2022-02-28},
  abstract = {Artificial Intelligence (AI) is implemented in various applications of telecommunication domain, ranging from managing the network, controlling a specific hardware function, preventing a failure, or troubleshooting a problem till automating the network slice management in 5G. The greater levels of autonomy increase the need for explainability of the decisions made by AI so that humans can understand them (e.g. the underlying data evidence and causal reasoning) consequently enabling trust. This paper presents first, the application of multiple global and local explainability methods with the main purpose to analyze the root-cause of Service Level Agreement violation prediction in a 5G network slicing setup by identifying important features contributing to the decision. Second, it performs a comparative analysis of the applied methods to analyze explainability of the predicted violation. Further, the global explainability results are validated using statistical Causal Dataframe method in order to improve the identified cause of the problem and thus validating the explainability.},
  eventtitle = {{{GLOBECOM}} 2020 - 2020 {{IEEE Global Communications Conference}}},
  isbn = {978-1-7281-8298-8},
  langid = {english},
  file = {/Users/erugfra/Library/CloudStorage/GoogleDrive-franco.ruggeri.pro@gmail.com/My Drive/3-resources/literature/terra-2020.pdf;/Users/erugfra/Zotero/storage/J5PEQYYF/terra-2020.pdf}
}

@article{terra-2022-beerl-both-ends,
  title = {{{BEERL}}: {{Both Ends Explanations}} for {{Reinforcement Learning}}},
  shorttitle = {{{BEERL}}},
  author = {Terra, Ahmad and Inam, Rafia and Fersman, Elena},
  date = {2022-10-28},
  journaltitle = {Applied Sciences},
  shortjournal = {Applied Sciences},
  volume = {12},
  number = {21},
  pages = {10947},
  issn = {2076-3417},
  doi = {10.3390/app122110947},
  url = {https://www.mdpi.com/2076-3417/12/21/10947},
  urldate = {2022-11-21},
  abstract = {Deep Reinforcement Learning (RL) is a black-box method and is hard to understand because the agent employs a neural network (NN). To explain the behavior and decisions made by the agent, different eXplainable RL (XRL) methods are developed; for example, feature importance methods are applied to analyze the contribution of the input side of the model, and reward decomposition methods are applied to explain the components of the output end of the RL model. In this study, we present a novel method to connect explanations from both input and output ends of a black-box model, which results in fine-grained explanations. Our method exposes the reward prioritization to the user, which in turn generates two different levels of explanation and allows RL agent reconfigurations when unwanted behaviors are observed. The method further summarizes the detailed explanations into a focus value that takes into account all reward components and quantifies the fulfillment of the explanation of desired properties. We evaluated our method by applying it to a remote electrical telecom-antenna-tilt use case and two openAI gym environments: lunar lander and cartpole. The results demonstrated fine-grained explanations by detailing input features’ contributions to certain rewards and revealed biases of the reward components, which are then addressed by adjusting the reward’s weights.},
  langid = {english}
}
