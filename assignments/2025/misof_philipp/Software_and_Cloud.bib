@inproceedings{cabralInvestigatingImpactSOLID2024,
  title = {Investigating the {{Impact}} of {{SOLID Design Principles}} on {{Machine Learning Code Understanding}}},
  booktitle = {Proceedings of the {{IEEE}}/{{ACM}} 3rd {{International Conference}} on {{AI Engineering}} - {{Software Engineering}} for {{AI}}},
  author = {Cabral, Raphael and Kalinowski, Marcos and Baldassarre, Maria Teresa and Villamizar, Hugo and Escovedo, Tatiana and Lopes, Hélio},
  date = {2024-06-11},
  series = {{{CAIN}} '24},
  pages = {7--17},
  publisher = {Association for Computing Machinery},
  location = {New York, NY, USA},
  doi = {10.1145/3644815.3644957},
  url = {https://dl.acm.org/doi/10.1145/3644815.3644957},
  urldate = {2025-08-25},
  abstract = {[Context] Applying design principles has long been acknowledged as beneficial for understanding and maintainability in traditional software projects. These benefits may similarly hold for Machine Learning (ML) projects, which involve iterative experimentation with data, models, and algorithms. However, ML components are often developed by data scientists with diverse educational backgrounds, potentially resulting in code that doesn't adhere to software design best practices. [Goal] In order to better understand this phenomenon, we investigated the impact of the SOLID design principles on ML code understanding. [Method] We conducted a controlled experiment with three independent trials involving 100 data scientists. We restructured real industrial ML code that did not use SOLID principles. Within each trial, one group was presented with the original ML code, while the other was presented with ML code incorporating SOLID principles. Participants of both groups were asked to analyze the code and fill out a questionnaire that included both open-ended and closed-ended questions on their understanding. [Results] The study results provide statistically significant evidence that the adoption of the SOLID design principles can improve code understanding within the realm of ML projects. [Conclusion] We put forward that software engineering design principles should be spread within the data science community and considered for enhancing the maintainability of ML code.},
  isbn = {979-8-4007-0591-5},
  file = {/home/philipp/Zotero/storage/DJ5KEHC8/Cabral et al. - 2024 - Investigating the Impact of SOLID Design Principles on Machine Learning Code Understanding.pdf}
}

@inproceedings{sculleyHiddenTechnicalDebt2015,
  title = {Hidden {{Technical Debt}} in {{Machine Learning Systems}}},
  booktitle = {Advances in {{Neural Information Processing Systems}}},
  author = {Sculley, D. and Holt, Gary and Golovin, Daniel and Davydov, Eugene and Phillips, Todd and Ebner, Dietmar and Chaudhary, Vinay and Young, Michael and Crespo, Jean-François and Dennison, Dan},
  date = {2015},
  volume = {28},
  publisher = {Curran Associates, Inc.},
  url = {https://proceedings.neurips.cc/paper_files/paper/2015/hash/86df7dcfd896fcaf2674f757a2463eba-Abstract.html},
  urldate = {2025-06-05},
  abstract = {Machine learning offers a fantastically powerful toolkit for building useful complexprediction systems quickly. This paper argues it is dangerous to think ofthese quick wins as coming for free. Using the software engineering frameworkof technical debt, we find it is common to incur massive ongoing maintenancecosts in real-world ML systems. We explore several ML-specific risk factors toaccount for in system design. These include boundary erosion, entanglement,hidden feedback loops, undeclared consumers, data dependencies, configurationissues, changes in the external world, and a variety of system-level anti-patterns.},
  file = {/home/philipp/Zotero/storage/LXNUYL66/Sculley et al. - 2015 - Hidden Technical Debt in Machine Learning Systems.pdf}
}

@article{serbanSoftwareEngineeringPractices2024,
  title = {Software Engineering Practices for Machine Learning — {{Adoption}}, Effects, and Team Assessment},
  author = {Serban, Alex and family=Blom, given=Koen, prefix=van der, useprefix=true and Hoos, Holger and Visser, Joost},
  date = {2024-03-01},
  journaltitle = {Journal of Systems and Software},
  shortjournal = {Journal of Systems and Software},
  volume = {209},
  pages = {111907},
  issn = {0164-1212},
  doi = {10.1016/j.jss.2023.111907},
  url = {https://www.sciencedirect.com/science/article/pii/S0164121223003023},
  urldate = {2025-06-05},
  abstract = {Machine learning (ML) is extensively used in production-ready applications, calling for mature engineering techniques to ensure robust development, deployment and maintenance. Given the potential negative impact machine learning (ML) can have on people, society or the environment, engineering techniques that can ensure robustness against technical errors and adversarial attacks are of considerable importance. In this work, we investigate how teams of experts develop, deploy and maintain software with ML components. Moreover, we link what teams do to the effects they aim to achieve and provide means for improvement. Towards this goal, we performed a mixed-methods study with a sequential exploratory strategy. First, we performed a systematic literature review through which we mined both academic and grey literature, and compiled a catalogue of engineering practices for ML. Second, we validated this catalogue using a large-scale survey, which measured the degree of adoption of the practices and their perceived effects. Third, we ran validation interviews with practitioners to add depth to the survey results. The catalogue covers a broad range of practices for engineering software systems with ML components and for ensuring non-functional properties that fall under the umbrella of trustworthy ML, such as fairness, security or accountability. Here, we present the results of our study, which indicate, for example, that larger and more experienced teams tend to adopt more practices, but that trustworthiness practices tend to be neglected. Moreover, we show that the effects measured in our survey, such as team agility or accountability, can be predicted quite accurately from groups of practices. This allowed us to contrast the importance of the practices for these effects as well as adoption rates, revealing, for example, that widely adopted practices are, in reality, less important with respect to some effects. For instance, writing reusable scripts for data cleaning and merging is highly adopted, but has a limited impact on reproducibility. Overall, our study provides a quantitative assessment of ML engineering practices and their impact on desirable properties of software with ML components, by which we open multiple avenues for improving the adoption of useful practices. Editor’s note: Open Science material was validated by the Journal of Systems and Software Open Science Board.},
  keywords = {Artificial Intelligence,Engineering practices,Machine learning,Maturity Models,Software engineering},
  file = {/home/philipp/Zotero/storage/UVFSNP35/Serban et al. - 2024 - Software engineering practices for machine learning — Adoption, effects, and team assessment.pdf;/home/philipp/Zotero/storage/3ZDX9HXB/S0164121223003023.html}
}

@inproceedings{ximenesInvestigatingIssuesThat2025,
  title = {Investigating {{Issues That Lead}} to {{Code Technical Debt}} in {{Machine Learning Systems}}},
  booktitle = {2025 {{IEEE}}/{{ACM}} 4th {{International Conference}} on {{AI Engineering}} – {{Software Engineering}} for {{AI}} ({{CAIN}})},
  author = {Ximenes, Rodrigo and Alves, Antonio Pedro Santos and Escovedo, Tatiana and Spinola, Rodrigo and Kalinowski, Marcos},
  date = {2025-04},
  pages = {173--183},
  doi = {10.1109/CAIN66642.2025.00028},
  url = {https://ieeexplore.ieee.org/abstract/document/11030012},
  urldate = {2025-08-25},
  abstract = {[Context] Technical debt (TD) in machine learning (ML) systems, much like its counterpart in software engineering (SE), holds the potential to lead to future rework, posing risks to productivity, quality, and team morale. Despite growing attention to TD in SE, the understanding of ML-specific code-related TD remains underexplored. [Objective] This paper aims to identify and discuss the relevance of code-related issues that lead to TD in ML code throughout the ML workflow. [Method] The study first compiled a list of 34 potential issues contributing to TD in ML code by examining the phases of the ML workflow, their typical associated activities, and problem types. This list was refined through two focus group sessions involving nine experienced ML professionals, where each issue was assessed based on its occurrence contributing to TD in ML code and its relevance. [Results] The list of issues contributing to TD in the source code of ML systems was refined from 34 to 30, with 24 of these issues considered highly relevant. The data pre-processing phase was the most critical, with 14 issues considered highly relevant. Shortcuts in code related to typical pre-processing tasks (e.g., handling missing values, outliers, inconsistencies, scaling, rebalancing, and feature selection) often result in “patch fixes” rather than sustainable solutions, leading to the accumulation of TD and increasing maintenance costs. Relevant issues were also found in the data collection, model creation and training, and model evaluation phases. [Conclusion] We have made the final list of issues available to the community and believe it will help raise awareness about issues that need to be addressed throughout the ML workflow to reduce TD and improve the maintainability of ML code.},
  eventtitle = {2025 {{IEEE}}/{{ACM}} 4th {{International Conference}} on {{AI Engineering}} – {{Software Engineering}} for {{AI}} ({{CAIN}})},
  keywords = {Codes,Data collection,Data models,Focus Group,Machine learning,Machine Learning,Maintenance,Object recognition,Productivity,Software engineering,Source coding,Technical Debt,Training},
  file = {/home/philipp/Zotero/storage/R7KNC57M/Ximenes et al. - 2025 - Investigating Issues That Lead to Code Technical Debt in Machine Learning Systems.pdf}
}
