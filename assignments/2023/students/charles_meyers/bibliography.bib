@inproceedings{ben2020adversarial,
  title={The adversarial robustness of sampling},
  author={Ben-Eliezer, Omri and Yogev, Eylon},
  booktitle={Proceedings of the 39th ACM SIGMOD-SIGACT-SIGAI Symposium on Principles of Database Systems},
  pages={49--62},
  year={2020}
}

@article{christmann2004robustness,
  title={On robustness properties of convex risk minimization methods for pattern recognition},
  author={Christmann, Andreas and Steinwart, Ingo},
  journal={The Journal of Machine Learning Research},
  volume={5},
  pages={1007--1034},
  year={2004},
  publisher={JMLR. org}
}

@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew and Doshi-Velez, Finale},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}

@misc{iso26262,
    author={International Standards Organization},
    title={{ISO} 26262-1:2011, Road vehicles --- Functional safety},
    howpublished={\href{https://www.iso.org/standard/43464.html}{https://www.iso.org/standard/43464.html} (visited 2022-04-20)},
    year={2018},
}

@inproceedings{jakubovitz2018improving,
  title={Improving dnn robustness to adversarial attacks using jacobian regularization},
  author={Jakubovitz, Daniel and Giryes, Raja},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={514--529},
  year={2018}
}

@article{falco2006using,
  title={Using host-based anti-virus software on industrial control systems: Integration guidance and a test methodology for assessing performance impacts},
  author={Falco, Joseph A and Hurd, Steve and Teumim, Dave},
  year={2006},
  publisher={Joseph A. Falco, S Hurd, D Teumim}
}

@article{firenet,
  title={Can stable and accurate neural networks be computed},
  author={Colbrook, Matthew~J. and Antun, Vegard and Hansen, Anders~C.},
  journal={On the barriers of deep learning and Smale’s 18th problem. arXiv},
  volume={2101},
  year={2021}
}



@article{bienstock2018principled,
  title={Principled deep neural network training through linear programming},
  author={Bienstock, Daniel and Mu{\~n}oz, Gonzalo and Pokutta, Sebastian},
  journal={arXiv:1810.03218},
  year={2018}
}

@article{de2010optimized,
  title={Optimized fixed-size kernel models for large data sets},
  author={De Brabanter, Kris and De Brabanter, Jos and Suykens, Johan AK and De Moor, Bart},
  journal={Computational Statistics \& Data Analysis},
  volume={54},
  number={6},
  pages={1484--1504},
  year={2010},
  publisher={Elsevier}
}

@article{fan2008liblinear,
  title={LIBLINEAR: A library for large linear classification},
  author={Fan, Rong-En and Chang, Kai-Wei and Hsieh, Cho-Jui and Wang, Xiang-Rui and Lin, Chih-Jen},
  journal={the Journal of machine Learning research},
  volume={9},
  pages={1871--1874},
  year={2008},
  publisher={JMLR. org}
}


@article{scikit-learn,
 title={Scikit-learn: Machine Learning in {P}ython},
 author={Pedregosa, F. and Varoquaux, G. and Gramfort, A. and Michel, V.
         and Thirion, B. and Grisel, O. and Blondel, M. and Prettenhofer, P.
         and Weiss, R. and Dubourg, V. and Vanderplas, J. and Passos, A. and
         Cournapeau, D. and Brucher, M. and Perrot, M. and Duchesnay, E.},
 journal={Journal of Machine Learning Research},
 volume={12},
 pages={2825--2830},
 year={2011}
}

@Book{metric,
 author = {Searcóid, Mícheál},
 title = {Metric spaces},
 publisher = {Springer},
 year = {2006},
 address = {London},
 isbn = {978-1-84628-369-7}
 }

@inproceedings{demontis2019adversarial,
  title={Why do adversarial attacks transfer? explaining transferability of evasion and poisoning attacks},
  author={Demontis, Ambra and Melis, Marco and Pintor, Maura and Jagielski, Matthew and Biggio, Battista and Oprea, Alina and Nita-Rotaru, Cristina and Roli, Fabio},
  booktitle={28th $\{$USENIX$\}$ Security Symposium},
  pages={321--338},
  year={2019}
}

@incollection{tzotsos2008support,
  title={Support vector machine classification for object-based image analysis},
  author={Tzotsos, Angelos and Argialas, Demetre},
  booktitle={Object-Based Image Analysis},
  pages={663--677},
  year={2008},
  publisher={Springer}
}

@inproceedings{zhang2019theoretically,
  title={Theoretically principled trade-off between robustness and accuracy},
  author={Zhang, Hongyang and Yu, Yaodong and Jiao, Jiantao and Xing, Eric and El Ghaoui, Laurent and Jordan, Michael},
  booktitle={International Conference on Machine Learning},
  pages={7472--7482},
  year={2019},
  organization={PMLR}
}

@misc{Dua:2019 ,
author = "Dua, Dheeru and Graff, Casey",
year = "2017",
title = "{UCI} Machine Learning Repository",
url = "http://archive.ics.uci.edu/ml",
institution = "University of California, Irvine, School of Information and Computer Sciences" } 

@article{chang2011libsvm,
  title={LIBSVM: a library for support vector machines},
  author={Chang, Chih-Chung and Lin, Chih-Jen},
  journal={ACM transactions on intelligent systems and technology (TIST)},
  volume={2},
  number={3},
  pages={1--27},
  year={2011},
  publisher={Acm New York, NY, USA}
}


@article{carlini2019evaluating,
  title={On evaluating adversarial robustness},
  author={Carlini, Nicholas and Athalye, Anish and Papernot, Nicolas and Brendel, Wieland and Rauber, Jonas and Tsipras, Dimitris and Goodfellow, Ian and Madry, Aleksander and Kurakin, Alexey},
  journal={arXiv:1902.06705},
  year={2019}
}

@InProceedings{ matthew_rocklin-proc-scipy-2015,
  author    = { Matthew Rocklin },
  title     = { Dask: Parallel Computation with Blocked algorithms and Task Scheduling },
  booktitle = { Proceedings of the 14th Python in Science Conference },
  pages     = { 130 - 136 },
  year      = { 2015 },
  editor    = { Kathryn Huff and James Bergstra }
}


@inproceedings{mehmood2015svm,
  title={SVM for network anomaly detection using ACO feature subset},
  author={Mehmood, Tahir and Rais, Helmi B Md},
  booktitle={2015 International symposium on mathematical sciences and computing research (iSMSC)},
  pages={121--126},
  year={2015},
  organization={IEEE}
}

@inproceedings{kim2003network,
  title={Network-based intrusion detection with support vector machines},
  author={Kim, Dong Seong and Park, Jong Sou},
  booktitle={International Conference on Information Networking},
  pages={747--756},
  year={2003},
  organization={Springer}
}

@article{bect2017bayesian,
  title={Bayesian subset simulation},
  author={Bect, Julien and Li, Ling and Vazquez, Emmanuel},
  journal={SIAM/ASA Journal on Uncertainty Quantification},
  volume={5},
  number={1},
  pages={762--786},
  year={2017},
  publisher={SIAM}
}


@inproceedings{carlini2017towards,
  title={Towards evaluating the robustness of neural networks},
  author={Carlini, Nicholas and Wagner, David},
  booktitle={IEEE symposium on security and privacy (sp)},
  pages={39--57},
  year={2017},
  organization={IEEE}
}

@article{li2016general,
  title={A general retraining framework for scalable adversarial classification},
  author={Li, Bo and Vorobeychik, Yevgeniy and Chen, Xinyun},
  journal={Workshop on Adversarial Training, Neural Information Processing Systems},
  year={2016}
}

@inproceedings{biggio2013evasion,
  title={Evasion attacks against machine learning at test time},
  author={Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and {\v{S}}rndi{\'c}, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
  booktitle={Joint European conference on machine learning and knowledge discovery in databases},
  pages={387--402},
  year={2013},
  organization={Springer}
}

@inproceedings{simon2019first,
  title={First-order adversarial vulnerability of neural networks and input dimension},
  author={Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Leon and Sch{\"o}lkopf, Bernhard and Lopez-Paz, David},
  booktitle={International Conference on Machine Learning},
  pages={5809--5817},
  year={2019},
  organization={PMLR}
}

@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={International Conference on Machine Learning},
  year={2017}
}

@inproceedings{fredrikson2015model,
  title={Model inversion attacks that exploit confidence information and basic countermeasures},
  author={Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
  booktitle={Proceedings of the 22nd ACM SIGSAC Conference on Computer and Communications Security},
  pages={1322--1333},
  year={2015}
}



@inproceedings{dohmatob2019generalized,
  title={Generalized no free lunch theorem for adversarial robustness},
  author={Dohmatob, Elvis},
  booktitle={International Conference on Machine Learning},
  pages={1646--1654},
  year={2019},
  organization={PMLR}
}

@article{szegedy2013intriguing,
  title={Intriguing properties of neural networks},
  author={Szegedy, Christian and Zaremba, Wojciech and Sutskever, Ilya and Bruna, Joan and Erhan, Dumitru and Goodfellow, Ian and Fergus, Rob},
  journal={International Conference on Learning Representations},
  year={2013}
}

@article{stutz2019confidence,
  title={Confidence-Calibrated Adversarial Training: Towards Robust Models Generalizing Beyond the Attack Used During Training},
  author={Stutz, David and Hein, Matthias and Schiele, Bernt},
  journal={International Conference on Machine Learning},
  year={2019}
}

@article{tsipras2018robustness,
  title={Robustness may be at odds with accuracy},
  author={Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
  journal={Int'l Conference on Learning Representations},
  year={2018}
}


@article{athalye2018obfuscated,
  title={Obfuscated gradients give a false sense of security: Circumventing defenses to adversarial examples},
  author={Athalye, Anish and Carlini, Nicholas and Wagner, David},
  journal={Int. Conference on Machine Learning},
  year={2018}
}

@article{uesato2018adversarial,
  title={Adversarial risk and the dangers of evaluating against weak attacks},
  author={Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
  journal={Proceedings of Machine Learning Research},
  year={2018}
}

@article{cortes1995support,
  title={Support-vector networks},
  author={Cortes, Corinna and Vapnik, Vladimir},
  journal={Machine learning},
  volume={20},
  number={3},
  pages={273--297},
  year={1995},
  publisher={Springer}
}

@article{trafalis2007robust,
  title={Robust support vector machines for classification and computational issues},
  author={Trafalis, Theodore B and Gilbert, Robin C},
  journal={Optimisation Methods and Software},
  volume={22},
  number={1},
  pages={187--198},
  year={2007},
  publisher={Taylor \& Francis}
}

@article{bordes2005fast,
  title={Fast kernel classifiers with online and active learning},
  author={Bordes, Antoine and Ertekin, Seyda and Weston, Jason and Bottou, L{\'e}on},
  journal={Journal of Machine Learning Research},
  volume={6},
  number={Sep},
  pages={1579--1619},
  year={2005}
}

@article{bottou2007support,
  title={Support vector machine solvers},
  author={Bottou, L{\'e}on and Lin, Chih-Jen},
  journal={Large scale kernel machines},
  volume={3},
  number={1},
  pages={301--320},
  year={2007},
  publisher={MIT press Cambridge, MA}
}

@article{croce2020reliable,
  title={Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
  author={Croce, Francesco and Hein, Matthias},
  journal={International Conference on Machine Learning},
  year={2020}
}

@article{biggio2012poisoning,
  title={Poisoning attacks against support vector machines},
  author={Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
  journal={International Conference on Machine Learning},
  year={2012}
}

@article{grosse2018limitations,
  title={The limitations of model uncertainty in adversarial settings},
  author={Grosse, Kathrin and Pfaff, David and Smith, Michael Thomas and Backes, Michael},
  journal={arXiv:1812.02606},
  year={2018}
}

@article{kotyan2019adversarial,
  title={Adversarial Robustness Assessment: Why both $L_0 $ and $L_\infty$ Attacks Are Necessary},
  author={Kotyan, Shashank and Vargas, Danilo Vasconcellos},
  journal={arXiv:1906.06026},
  year={2019}
}

@inproceedings{chen2017zoo,
  title={Zoo: Zeroth order optimization based black-box attacks to deep neural networks without training substitute models},
  author={Chen, Pin-Yu and Zhang, Huan and Sharma, Yash and Yi, Jinfeng and Hsieh, Cho-Jui},
  booktitle={Proceedings of the 10th ACM workshop on artificial intelligence and security},
  pages={15--26},
  year={2017}
}

@article{brown2017adversarial,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv:1712.09665},
  year={2017}
}

@article{liu2018dpatch,
  title={Dpatch: An adversarial patch attack on object detectors},
  author={Liu, Xin and Yang, Huanrui and Liu, Ziwei and Song, Linghao and Li, Hai and Chen, Yiran},
  journal={arXiv:1806.02299},
  year={2018}
}

@inproceedings{qin2019imperceptible,
  title={Imperceptible, robust, and targeted adversarial examples for automatic speech recognition},
  author={Qin, Yao and Carlini, Nicholas and Cottrell, Garrison and Goodfellow, Ian and Raffel, Colin},
  booktitle={International conference on machine learning},
  pages={5231--5240},
  year={2019},
  organization={PMLR}
}

@article{brendel2017decision,
  title={Decision-based adversarial attacks: Reliable attacks against black-box machine learning models},
  author={Brendel, Wieland and Rauber, Jonas and Bethge, Matthias},
  journal={arXiv:1712.04248},
  year={2017}
}

@article{ozaki2022multiobjective,
  title={Multiobjective tree-structured Parzen estimator},
  author={Ozaki, Yoshihiko and Tanigaki, Yuki and Watanabe, Shuhei and Nomura, Masahiro and Onishi, Masaki},
  journal={Journal of Artificial Intelligence Research},
  volume={73},
  pages={1209--1250},
  year={2022}
}

@inproceedings{chen2020hopskipjumpattack,
  title={Hopskipjumpattack: A query-efficient decision-based attack},
  author={Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
  booktitle={2020 ieee symposium on security and privacy (sp)},
  pages={1277--1294},
  year={2020},
  organization={IEEE}
}

@article{li2017hyperband,
  title={Hyperband: A novel bandit-based approach to hyperparameter optimization},
  author={Li, Lisha and Jamieson, Kevin and DeSalvo, Giulia and Rostamizadeh, Afshin and Talwalkar, Ameet},
  journal={The Journal of Machine Learning Research},
  volume={18},
  number={1},
  pages={6765--6816},
  year={2017},
  publisher={JMLR. org}
}

@article{su2019one,
  title={One pixel attack for fooling deep neural networks},
  author={Su, Jiawei and Vargas, Danilo Vasconcellos and Sakurai, Kouichi},
  journal={IEEE Transactions on Evolutionary Computation},
  volume={23},
  number={5},
  pages={828--841},
  year={2019},
  publisher={IEEE}
}

@article{hansen2016cma,
  title={The CMA evolution strategy: A tutorial},
  author={Hansen, Nikolaus},
  journal={arXiv:1604.00772},
  year={2016}
}

@article{raghunathan2020understanding,
  title={Understanding and mitigating the tradeoff between robustness and accuracy},
  author={Raghunathan, Aditi and Xie, Sang Michael and Yang, Fanny and Duchi, John and Liang, Percy},
  journal={International Conference on Machine Learning},
  year={2020}
}


@article{xu2009robustness,
  title={Robustness and Regularization of Support Vector Machines.},
  author={Xu, Huan and Caramanis, Constantine and Mannor, Shie},
  journal={Journal of machine learning research},
  volume={10},
  number={7},
  year={2009}
}

@article{desislavov2021compute,
  title={Compute and energy consumption trends in deep learning inference},
  author={Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={arXiv:2109.05472},
  year={2021}
}

@article{bertsimas2019robust,
  title={Robust classification},
  author={Bertsimas, Dimitris and Dunn, Jack and Pawlowski, Colin and Zhuo, Ying Daisy},
  journal={INFORMS Journal on Optimization},
  volume={1},
  number={1},
  pages={2--34},
  year={2019},
  publisher={INFORMS}
}


@article{wang2014iteration,
  title={Iteration complexity of feasible descent methods for convex optimization},
  author={Wang, Po-Wei and Lin, Chih-Jen},
  journal={The Journal of Machine Learning Research},
  volume={15},
  number={1},
  pages={1523--1548},
  year={2014},
  publisher={JMLR. org}
}

@article{Meidan_2018,
   title={{N-BaIoT}—Network-Based Detection of {IoT} Botnet Attacks Using Deep Autoencoders},
   volume={17},
   number={3},
   journal={IEEE Pervasive Computing},
   publisher={Institute of Electrical and Electronics Engineers (IEEE)},
   author={Meidan, Yair and Bohadana, Michael and Mathov, Yael and Mirsky, Yisroel and Shabtai, Asaf and Breitenbacher, Dominik and Elovici, Yuval},
   year={2018},
   month={Jul},
   pages={12–22}
}

@article{liu_2020,
   title={{MadDroid}: Characterizing and Detecting Devious Ad Contents for Android Apps},
   journal={Proceedings of The Web Conference 2020},
   publisher={ACM},
   author={Liu, Tianming and Wang, Haoyu and Li, Li and Luo, Xiapu and Dong, Feng and Guo, Yao and Wang, Liu and Bissyandé, Tegawendé and Klein, Jacques},
   year={2020},
   month={Apr}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}

@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction apis},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th $\{$USENIX$\}$ Security Symposium Security 16)},
  pages={601--618},
  year={2016}
}

@article{ateniese2015hacking,
  title={Hacking smart machines with smarter ones: How to extract meaningful data from machine learning classifiers},
  author={Ateniese, Giuseppe and Mancini, Luigi V and Spognardi, Angelo and Villani, Antonio and Vitali, Domenico and Felici, Giovanni},
  journal={International Journal of Security and Networks},
  volume={10},
  number={3},
  pages={137--150},
  year={2015},
  publisher={Inderscience Publishers (IEL)}
}

@article{wang2019security,
  title={The security of machine learning in an adversarial setting: A survey},
  author={Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
  journal={Journal of Parallel and Distributed Computing},
  volume={130},
  pages={12--23},
  year={2019},
  publisher={Elsevier}
}

@article{miller2020adversarial,
  title={Adversarial learning targeting deep neural network classification: A comprehensive review of defenses against attacks},
  author={Miller, David J and Xiang, Zhen and Kesidis, George},
  journal={Proceedings of the IEEE},
  volume={108},
  number={3},
  pages={402--433},
  year={2020},
  publisher={IEEE}
}

@article{chakraborty2018adversarial,
  title={Adversarial attacks and defences: A survey},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={arXiv:1810.00069},
  year={2018}
}

@article{jagielski2018manipulate,
  title={Manipulating machine learning: Poisoning attacks and countermeasures for regression learning},
  author={Jagielski, Matthew, Alina Oprea, Battista Biggio, Chang Liu, Cristina Nita-Rotaru, and Bo Li},
  journal={IEEE Symposium on Security and Privacy (SP)},
  year={2018}
}
@article{kotyan2022adversarial,
  title={Adversarial robustness assessment: Why in evaluation both L0 and L∞ attacks are necessary},
  author={Kotyan, Shashank and Vargas, Danilo Vasconcellos},
  journal={PLoS ONE},
  volume={17},
  number={4},
  pages={e0265723},
  year={2022},
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press, Cambridge, UK.},
}

@article{blumer1989learnability,
  title={Learnability and the Vapnik-Chervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM},
  volume={36},
  number={4},
  pages={929--965},
  year={1989},
}

@article{aft_models,
  title={Survival analysis part II: multivariate data analysis--an introduction to concepts and methods},
  author={Bradburn, Mike J and Clark, Taane G and Love, Sharon B and Altman, Douglas Graham},
  journal={British journal of cancer},
  volume={89},
  number={3},
  pages={431--436},
  year={2003},
  publisher={Nature Publishing Group}
}

@article{hadj2018continuation,
  title={Continuation of {Nesterov}’s smoothing for regression with structured sparsity in high-dimensional neuroimaging},
  author={Hadj-Selem, Fouad and L{\"o}fstedt, Tommy and Dohmatob, Elvis and Frouin, Vincent and Dubois, Mathieu and Guillemot, Vincent and Duchesnay, Edouard},
  journal={IEEE Transactions on Medical Imaging},
  volume={37},
  number={11},
  pages={2403--2413},
  year={2018},
  publisher={IEEE}
}

@inbook{hoffstein_pipher_silverman_2010,
    title={Complexity Theory and P vs NP},
    booktitle={An introduction to mathematical cryptography},
    publisher={Springer},
    author={Hoffstein, Jeffrey and Pipher, Jill and Silverman, Joseph H.},
    year={2010},
    chapter={4},
    pages={258-262},
}


 @book{IEC61508, edition={2nd}, title={IEC 61508 Safety and Functional Safety}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2010}} 
 
 @book{IEC62034, edition={2nd}, title={IEC 62304 Medical Device Software - Software Life Cycle Processes}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2006}} 

@article{bect_bayesian_2017,
	title = {Bayesian subset simulation},
	volume = {5},
	issn = {2166-2525},
	url = {http://arxiv.org/abs/1601.02557},
	doi = {10.1137/16M1078276},
	abstract = {We consider the problem of estimating a probability of failure α, deﬁned as the volume of the excursion set of a function f : X ⊆ Rd → R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate α when the number of evaluations of f is very limited and α is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to deﬁne the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves signiﬁcant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
	language = {en},
	number = {1},
	urldate = {2020-07-20},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
	month = jan,
	year = {2017},
	keywords = {Statistics - Computation},
	pages = {762--786},
	file = {Bect et al. - 2017 - Bayesian subset simulation.pdf:C\:\\Users\\charlie\\Zotero\\storage\\CFEQS6UF\\Bect et al. - 2017 - Bayesian subset simulation.pdf:application/pdf}
}

@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}

@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}


@article{desislavov2021compute,
  title={Compute and energy consumption trends in deep learning inference},
  author={Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={arXiv:2109.05472},
  year={2021}
}


@article{roh2019survey,
  title={A survey on data collection for machine learning: a big data-ai integration perspective},
  author={Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={4},
  pages={1328--1347},
  year={2019},
  publisher={IEEE}
}

@article{sehwag2019towards,
  title={Towards compact and robust deep neural networks},
  author={Sehwag, Vikash and Wang, Shiqi and Mittal, Prateek and Jana, Suman},
  journal={arXiv:1906.06110},
  year={2019}
}


@inproceedings{jakubovitz2018improving,
  title={Improving dnn robustness to adversarial attacks using jacobian regularization},
  author={Jakubovitz, Daniel and Giryes, Raja},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={514--529},
  year={2018}
}

@article{colbrook2021can,
  title={Can stable and accurate neural networks be computed},
  author={Colbrook, Matthew~J. and Antun, Vegard and Hansen, Anders~C.},
  journal={On the barriers of deep learning and Smale’s 18th problem. arXiv},
  volume={2101},
  year={2021}
}

@inproceedings{sinn2019evolutionary,
  title={Evolutionary search for adversarially robust neural networks},
  author={Sinn, Mathieu and Wistuba, M and Buesser, B and Nicolae, MI and Tran, M},
  booktitle={Safe Machine Learning workshop at ICLR},
  year={2019}
}


@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew and Doshi-Velez, Finale},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}
% number={1},


@article{cosentino2019search,
  title={The search for sparse, robust neural networks},
  author={Cosentino, Justin and Zaiter, Federico and Pei, Dan and Zhu, Jun},
  journal={arXiv:1912.02386},
  year={2019}
}

@article{jian2022pruning,
  title={Pruning Adversarially Robust Neural Networks without Adversarial Examples},
  author={Jian, Tong and Wang, Zifeng and Wang, Yanzhi and Dy, Jennifer and Ioannidis, Stratis},
  journal={arXiv:2210.04311},
  year={2022}
}

@inproceedings{lam2004new,
  title={New design-to-test software strategies accelerate time-to-market},
  author={Lam, Hau},
  booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No. 04CH37585)},
  pages={140--143},
  year={2004},
  organization={IEEE}
}

@article{zirger1996effect,
  title={The effect of acceleration techniques on product development time},
  author={Zirger, Billie J and Hartley, Janet L},
  journal={IEEE Transactions on Engineering Management},
  volume={43},
  number={2},
  pages={143--152},
  year={1996},
  publisher={IEEE}
}

@book{ramirez2000resource,
  title={A resource guide on racial profiling data collection systems: Promising practices and lessons learned},
  author={Ramirez, Deborah and McDevitt, Jack and Farrell, Amy},
  year={2000},
  publisher={US Department of Justice}
}

@inproceedings{bloom2017self,
  title={Self-driving cars and data collection: Privacy perceptions of networked autonomous vehicles},
  author={Bloom, Cara and Tan, Joshua and Ramjohn, Javed and Bauer, Lujo},
  booktitle={Symposium on Usable Privacy and Security (SOUPS)},
  year={2017}
}

@article{gichoya2022ai,
  title={AI recognition of patient race in medical imaging: a modelling study},
  author={Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and Burns, John L and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and others},
  journal={The Lancet Digital Health},
  volume={4},
  number={6},
  pages={e406--e414},
  year={2022},
  publisher={Elsevier}
}

@article{evans2001gender,
  title={Gender and age influence on fatality risk from the same physical impact determined using two-car crashes},
  author={Evans, Leonard and Gerrish, Peter H},
  journal={SAE transactions},
  pages={1336--1341},
  year={2001},
  publisher={JSTOR}
}

@article{corsaro1982something,
  title={Something old and something new: The importance of prior ethnography in the collection and analysis of audiovisual data},
  author={Corsaro, William A},
  journal={Sociological Methods \& Research},
  volume={11},
  number={2},
  pages={145--166},
  year={1982},
  publisher={Sage Publications}
}

@article{banks2018driver,
  title={Driver error or designer error: Using the Perceptual Cycle Model to explore the circumstances surrounding the fatal Tesla crash on 7th May 2016},
  author={Banks, Victoria A and Plant, Katherine L and Stanton, Neville A},
  journal={Safety science},
  volume={108},
  pages={278--285},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018},
  organization={PMLR}
}

@article{lu2020gender,
  title={Gender bias in neural natural language processing},
  author={Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  journal={Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday},
  pages={189--202},
  year={2020},
  publisher={Springer}
}

@article{koch2021reduced,
  title={Reduced, reused and recycled: The life of a dataset in machine learning research},
  author={Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob G},
  journal={arXiv preprint arXiv:2112.01716},
  year={2021}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}


@inproceedings{ahn2003captcha,
  title={CAPTCHA: Using hard AI problems for security},
  author={Ahn, Luis von and Blum, Manuel and Hopper, Nicholas J and Langford, John},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={294--311},
  year={2003},
  organization={Springer}
}

@book{aviation,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{ma2020imbalanced,
  title={Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness},
  author={Ma, Xingjun and Jiang, Linxi and Huang, Hanxun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
  journal={arXiv:2006.13726},
  year={2020}
}


@article{chakraborty2018adversarial,
  title={Adversarial attacks and defences: A survey},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={arXiv:1810.00069},
  year={2018}
}

@inproceedings{braking,
  title={Autonomous braking system via deep reinforcement learning},
  author={Hyunmin Chae and Chang Mook Kang and ByeoungDo Kim and Jaekyum Kim and Chung Choo Chung and Jun Won Choi},
  booktitle={{IEEE} 20th International conference on intelligent transportation systems ({ITSC})},
  year={2017},
}

@inproceedings{cintas_detecting_2020,
	address = {Yokohama, Japan},
	title = {Detecting {Adversarial} {Attacks} via {Subset} {Scanning} of {Autoencoder} {Activations} and {Reconstruction} {Error}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/122},
	doi = {10.24963/ijcai.2020/122},
	abstract = {Reliably detecting attacks in a given set of inputs is of high practical relevance because of the vulnerability of neural networks to adversarial examples. These altered inputs create a security risk in applications with real-world consequences, such as self-driving cars, robotics and ﬁnancial services. We propose an unsupervised method for detecting adversarial attacks in inner layers of autoencoder (AE) networks by maximizing a non-parametric measure of anomalous node activations. Previous work in this space has shown AE networks can detect anomalous images by thresholding the reconstruction error produced by the ﬁnal layer. Furthermore, other detection methods rely on data augmentation or specialized training techniques which must be asserted before training time. In contrast, we use subset scanning methods from the anomalous pattern detection domain to enhance detection power without labeled examples of the noise, retraining or data augmentation methods. In addition to an anomalous “score” our proposed method also returns the subset of nodes within the AE network that contributed to that score. This will allow future work to pivot from detection to visualisation and explainability. Our scanning approach shows consistently higher detection power than existing detection methods across several adversarial noise models and a wide range of perturbation strengths.},
	language = {en},
	urldate = {2020-11-03},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Cintas, Celia and Speakman, Skyler and Akinwande, Victor and Ogallo, William and Weldemariam, Komminist and Sridharan, Srihari and McFowland, Edward},
	month = jul,
	year = {2020},
	pages = {876--882},
	file = {Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:C\:\\Users\\charlie\\Zotero\\storage\\RU2HGKV3\\Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:application/pdf}
}

@article{croce_reliable_2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	url = {http://arxiv.org/abs/2003.01690},
	abstract = {The ﬁeld of defense strategies against adversarial attacks has signiﬁcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufﬁcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difﬁcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we ﬁrst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:2003.01690 [cs, stat]},
	author = {Croce, Francesco and Hein, Matthias},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:C\:\\Users\\charlie\\Zotero\\storage\\R7F4DQN8\\Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:application/pdf}
}


@article{paudice_detection_2018,
	title = {Detection of {Adversarial} {Training} {Examples} in {Poisoning} {Attacks} through {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1802.03041},
	abstract = {Machine learning has become an important component for many systems and applications including computer vision, spam ﬁltering, malware and network intrusion detection, among others. Despite the capabilities of machine learning algorithms to extract valuable information from data and produce accurate predictions, it has been shown that these algorithms are vulnerable to attacks. Data poisoning is one of the most relevant security threats against machine learning systems, where attackers can subvert the learning process by injecting malicious samples in the training data. Recent work in adversarial machine learning has shown that the so-called optimal attack strategies can successfully poison linear classiﬁers, degrading the performance of the system dramatically after compromising a small fraction of the training dataset. In this paper we propose a defence mechanism to mitigate the effect of these optimal poisoning attacks based on outlier detection. We show empirically that the adversarial examples generated by these attack strategies are quite different from genuine points, as no detectability constrains are considered to craft the attack. Hence, they can be detected with an appropriate pre-ﬁltering of the training dataset.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1802.03041 [cs, stat]},
	author = {Paudice, Andrea and Muñoz-González, Luis and Gyorgy, Andras and Lupu, Emil C.},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:C\:\\Users\\charlie\\Zotero\\storage\\ABWQPGBP\\Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}


@article{daya_graph-based_2019,
	title = {A {Graph}-{Based} {Machine} {Learning} {Approach} for {Bot} {Detection}},
	url = {http://arxiv.org/abs/1902.08538},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1902.08538 [cs]},
	author = {Daya, Abbas Abou and Salahuddin, Mohammad A. and Limam, Noura and Boutaba, Raouf},
	month = feb,
	year = {2019},
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
	file = {Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:C\:\\Users\\charlie\\Zotero\\storage\\PDN3S6FR\\Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf}
}

@inproceedings{fredrikson_model_2015,
	address = {Denver, Colorado, USA},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	url = {http://dl.acm.org/citation.cfm?doid=2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} - {CCS} '15},
	publisher = {ACM Press},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	year = {2015},
	pages = {1322--1333},
	file = {Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:C\:\\Users\\charlie\\Zotero\\storage\\6A5APDC5\\Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:application/pdf}
}


@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{li_general_2016,
	title = {A {General} {Retraining} {Framework} for {Scalable} {Adversarial} {Classification}},
	url = {http://arxiv.org/abs/1604.02606},
	abstract = {Traditional classiﬁcation algorithms assume that training and test data come from similar distributions. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the ﬁrst systematic and general-purpose retraining framework which can: a) boost robustness of an arbitrary learning algorithm, in the face of b) a broader class of adversarial models than any prior methods. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but are more general and far more scalable. The experiments also conﬁrm that without retraining, our adversarial framework dramatically reduces the effectiveness of learning. In contrast, retraining signiﬁcantly boosts robustness to evasion attacks without signiﬁcantly compromising overall accuracy.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1604.02606 [cs, stat]},
	author = {Li, Bo and Vorobeychik, Yevgeniy and Chen, Xinyun},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:C\:\\Users\\charlie\\Zotero\\storage\\NGCPLE3C\\Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:application/pdf}
}

@article{biggio_evasion_2013,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but eﬀective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classiﬁcation algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit diﬀerent risk levels for the classiﬁer by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples. This gives the classiﬁer designer a better picture of the classiﬁer performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF ﬁles, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {387--402},
	file = {Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:C\:\\Users\\charlie\\Zotero\\storage\\KSVTHWRB\\Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {http://arxiv.org/abs/1802.00420},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1802.00420 [cs]},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:C\:\\Users\\charlie\\Zotero\\storage\\36AD8ZHI\\Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:application/pdf}
}



@INPROCEEDINGS{dohmatob_generalized_2019,
    author = {Dohmatob, Elvis},
    title = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	year = {2019},
    volume = {97},
    series = {PMLR},
}
% 1646-1654, 2019. 

@article{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1805.12152 [cs, stat]},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:C\:\\Users\\charlie\\Zotero\\storage\\2V7GJHIL\\Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}


@article{miller_adversarial_2020,
	title = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against} {Attacks}},
	volume = {108},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9013065/},
	doi = {10.1109/JPROC.2020.2970615},
	language = {en},
	number = {3},
	urldate = {2020-08-12},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	month = mar,
	year = {2020},
	pages = {402--433},
	file = {Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:C\:\\Users\\charlie\\Zotero\\storage\\VV3I4V2C\\Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:application/pdf}
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}



@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}

@article{biggio_evasion_2013-1,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but effective gradient-based approach that can be exploited to systematically assess the security of several, widely-used classification algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit different risk levels for the classifier by increasing the attacker's knowledge of the system and her ability to manipulate attack samples. This gives the classifier designer a better picture of the classifier performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF files, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	urldate = {2020-11-02},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {387--402},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\EUR3EQFC\\Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\S3HRVLZV\\1708.html:text/html}
}

@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}


@inproceedings{deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}


@article{pixelattack,
  title={Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary},
  author={Kotyan, Shashank and Vasconcellos Vargas, Danilo},
  journal={arXiv e-prints},
  pages={arXiv--1906},
  year={2019}
}

@article{finlayson2018adversarial,
  title={Adversarial Attacks Against Medical Deep Learning Systems},
  author={Finlayson, Samuel G and Chung, Hyung Won and Kohane, Isaac S and Beam, Andrew L},
  journal={arXiv:1804.05296},
  year={2018}
}

@inproceedings{hopskipjump,
  title={{HopSkipJumpAttack}: A query-efficient decision-based attack},
  author={Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
  booktitle={{IEEE} symposium on security and privacy (sp)},
  pages={1277--1294},
  year={2020},
  organization={IEEE}
}

@article{fgm,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv:1412.6572},
  year={2014}
}


@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv:1706.06083},
  year={2017}
}


@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}

@article{wang_security_2019,
	title = {The security of machine learning in an adversarial setting: {A} survey},
	volume = {130},
	issn = {07437315},
	shorttitle = {The security of machine learning in an adversarial setting},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518309183},
	doi = {10.1016/j.jpdc.2019.03.003},
	abstract = {Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models.},
	language = {en},
	urldate = {2020-08-12},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
	month = aug,
	year = {2019},
	pages = {12--23},
	file = {Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:C\:\\Users\\charlie\\Zotero\\storage\\IZAB3YMW\\Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:application/pdf}
}

@article{chakraborty_adversarial_2018,
	title = {Adversarial Attacks and Defences: {A} Survey},
	journal = {arXiv:1810.00069 [cs, stat]},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year = {2018},
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}

@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{biggio_multiple_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiple {Classifier} {Systems} for {Adversarial} {Classification} {Tasks}},
	isbn = {978-3-642-02326-2},
	doi = {10.1007/978-3-642-02326-2_14},
	abstract = {Pattern classification systems are currently used in security applications like intrusion detection in computer networks, spam filtering and biometric identity recognition. These are adversarial classification problems, since the classifier faces an intelligent adversary who adaptively modifies patterns (e.g., spam e-mails) to evade it. In these tasks the goal of a classifier is to attain both a high classification accuracy and a high hardness of evasion, but this issue has not been deeply investigated yet in the literature. We address it under the viewpoint of the choice of the architecture of a multiple classifier system. We propose a measure of the hardness of evasion of a classifier architecture, and give an analytical evaluation and comparison of an individual classifier and a classifier ensemble architecture. We finally report an experimental evaluation on a spam filtering task.},
	language = {en},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer},
	author = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	editor = {Benediktsson, Jón Atli and Kittler, Josef and Roli, Fabio},
	year = {2009},
	pages = {132--141}
}

@article{feature_squeezing,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv:1704.01155},
  year={2017}
}

@inproceedings{label_smoothing,
  title={Adversarial Perturbations of Deep Neural Networks},
  author={D. Warde-Farley and I. Goodfellow},
  editor={T. Hazan , G. Papandreou, D. Tarlow},
  booktitle={Perturbations, Optimization, and Statistics},
  publisher={The MIT Press, Cambridge, Massachusetts},
  year={2017},
}

@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction {API}s},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th {USENIX} Security Symposium ({USENIX} Security 16)},
  pages={601--618},
  year={2016}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}


@article{tramer,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}
@article{papernot,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}


@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@article{adversarialpatch,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv:1712.09665},
  year={2017}
}
@inproceedings{xiao2021improving,
  title={Improving Transferability of Adversarial Patches on Face Recognition With Generative Models},
  author={Xiao, Zihao and Gao, Xianfeng and Fu, Chilin and Dong, Yinpeng and Gao, Wei and Zhang, Xiaolu and Zhou, Jun and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11845--11854},
  year={2021}
}

@article{grigorescu2020survey,
  title={A survey of deep learning techniques for autonomous driving},
  author={Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
  journal={Journal of Field Robotics},
  volume={37},
  number={3},
  pages={362--386},
  year={2020},
  publisher={Wiley Online Library}
}

@book{nelson2010behavior,
  title={Behavior of machine learning algorithms in adversarial environments},
  author={Nelson, Blaine Alan},
  year={2010},
  publisher={University of California, Berkeley}
}


@inproceedings{al2017deep,
  title={Deep learning algorithm for autonomous driving using {GoogLeNet}},
  author={Al-Qizwini, Mohammed and Barjasteh, Iman and Al-Qassab, Hothaifa and Radha, Hayder},
  booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
  pages={89--96},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{gauss_out,
    author={Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
    booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
    title={Certified Robustness to Adversarial Examples with Differential Privacy},
    year={2019},
    pages={656-672},
}
% volume={},
% number={},

@ARTICLE{high_conf,  author={Chen, Li and Xiao, Jun and Zou, Pu and Li, Haifeng},  journal={IEEE Geoscience and Remote Sensing Letters},   title={Lie to Me: A Soft Threshold Defense Method for Adversarial Examples of Remote Sensing Images},   year={2021},  volume={},  number={},  pages={1-5},  doi={10.1109/LGRS.2021.3096244}}

@inproceedings{discretization,
  title={Defending against whitebox adversarial attacks via randomized discretization},
  author={Zhang, Yuchen and Liang, Percy},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={684--693},
  year={2019},
  organization={PMLR}
}


@inproceedings{tuan2010modeling,
  title={Modeling and verification of safety critical systems: A case study on pacemaker},
  author={Tuan, Luu Anh and Zheng, Man Chun and Tho, Quan Thanh},
  booktitle={2010 Fourth International Conference on Secure Software Integration and Reliability Improvement},
  pages={23--32},
  year={2010},
  organization={IEEE}
}

@inproceedings{aviation_software,
  title={Applying lessons from safety-critical systems to security-critical software},
  author={Axelrod, C Warren},
  booktitle={2011 IEEE Long Island Systems, Applications and Technology Conference},
  pages={1--6},
  year={2011},
  organization={IEEE}
}


@book{topologytextbook,
    place={New York},
    title={Introduction to topology},
    publisher={Dover Publications},
    author={Mendelson, Bert},
    year={2012},
}

@inproceedings{gauss_aug,
author = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
title = {Efficient Defenses Against Adversarial Attacks},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140449},
doi = {10.1145/3128572.3140449},
abstract = {Following the recent adoption of deep neural networks (DNN) accross a wide range of
applications, adversarial attacks against these models have proven to be an indisputable
threat. Adversarial samples are crafted with a deliberate intention of undermining
a system. In the case of DNNs, the lack of better understanding of their working has
prevented the development of efficient defenses. In this paper, we propose a new defense
method based on practical observations which is easy to integrate into models and
performs better than state-of-the-art defenses. Our proposed solution is meant to
reinforce the structure of a DNN, making its prediction more stable and less likely
to be fooled by adversarial samples. We conduct an extensive experimental study proving
the efficiency of our method against multiple attacks, comparing it to numerous defenses,
both in white-box and black-box setups. Additionally, the implementation of our method
brings almost no overhead to the training procedure, while maintaining the prediction
performance of the original model on clean samples.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {39–49}, 
numpages = {11},
keywords = {deep neural network, model security, defenses, adversarial learning},
location = {Dallas, Texas, USA},
series = {AISec '17}
}

@article{chambolle2004algorithm,
  title={An algorithm for total variation minimization and applications},
  author={Chambolle, Antonin},
  journal={Journal of Mathematical imaging and vision},
  volume={20},
  number={1},
  pages={89--97},
  year={2004},
  publisher={Springer}
}


@misc{iso26262,
    author={International Standards Organization},
    title={{ISO} 26262-1:2011, Road vehicles --- Functional safety},
    howpublished={\href{https://www.iso.org/standard/43464.html}{https://www.iso.org/standard/43464.html} (visited 2022-04-20)},
    year={2018},
}

@article{monmasson2011fpgas,
  title={FPGAs in industrial control applications},
  author={Monmasson, Eric and Idkhajine, Lahoucine and Cirstea, Marcian N and Bahri, Imene and Tisan, Alin and Naouar, Mohamed Wissem},
  journal={IEEE Transactions on Industrial informatics},
  volume={7},
  number={2},
  pages={224--243},
  year={2011},
  publisher={IEEE}
}


@article{rudin1992nonlinear,
  title={Nonlinear total variation based noise removal algorithms},
  author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  journal={Physica D: nonlinear phenomena},
  volume={60},
  number={1-4},
  pages={259--268},
  year={1992},
  publisher={Elsevier}
}


@article{fukuda1992theory,
  title={Theory and applications of neural networks for industrial control systems},
  author={Fukuda, Toshio and Shibata, Takanori},
  journal={IEEE Transactions on industrial electronics},
  volume={39},
  number={6},
  pages={472--489},
  year={1992},
  publisher={IEEE}
}

@article{resnet,
  author    = {Kaiming He and
               Xiangyu Zhang and
               Shaoqing Ren and
               Jian Sun},
  title     = {Deep Residual Learning for Image Recognition},
  journal   = {CoRR},
  volume    = {abs/1512.03385},
  year      = {2015},
  url       = {http://arxiv.org/abs/1512.03385},
  eprinttype = {arXiv},
  eprint    = {1512.03385},
  timestamp = {Wed, 17 Apr 2019 17:23:45 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/HeZRS15.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{reverse_sigmoid,
  author    = {Taesung Lee and
               Benjamin Edwards and
               Ian M. Molloy and
               Dong Su},
  title     = {Defending Against Model Stealing Attacks Using Deceptive Perturbations},
  journal   = {CoRR},
  volume    = {abs/1806.00054},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00054},
  eprinttype = {arXiv},
  eprint    = {1806.00054},
  timestamp = {Wed, 02 Jun 2021 09:13:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00054.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sahiner2019deep,
  title={Deep learning in medical imaging and radiation therapy},
  author={Sahiner, Berkman and Pezeshk, Aria and Hadjiiski, Lubomir M and Wang, Xiaosong and Drukker, Karen and Cha, Kenny H and Summers, Ronald M and Giger, Maryellen L},
  journal={Medical physics},
  volume={46},
  number={1},
  pages={e1--e36},
  year={2019},
  publisher={Wiley Online Library}
}

@book{pearson2005mining,
  title={Mining imperfect data: Dealing with contamination and incomplete records},
  author={Pearson, Ronald K},
  year={2005},
  publisher={SIAM}
}

@ARTICLE{ching2017opportunities,
    author = {Ching, Travers
              and Himmelstein, Daniel~S.
              and Beaulieu-Jones, Brett~K.
              and Kalinin, Alexandr~A.
              and Do, Brian~T.
              and Way, Gregory~P.
              and Ferrero, Enrico
              and Agapow, Paul-Michael
              and Zietz, Michael
              and Hoffman, Michael~M.
              and Xie, Wei
              and Rosen, Gail~L.
              and Lengerich, Benjamin~J.
              and Israeli, Johnny
              and Lanchantin, Jack
              and Woloszynek, Stephen
              and Carpenter, Anne~E.
              and Shrikumar, Avanti
              and Xu, Jinbo
              and Cofer, Evan~M.
              and Lavender, Christopher~A.
              and Turaga, Srinivas~C.
              and Alexandari, Amr~M.
              and Lu, Zhiyong
              and Harris, David~J.
              and DeCaprio, Dave
              and Qi, Yanjun
              and Kundaje, Anshul
              and Peng, Yifan
              and Wiley, Laura~K.
              and Segler, Marwin~H.~S.
              and Boca, Simina~M.
              and Swamidass, S.~ Joshua
              and Huang, Austin
              and Gitter, Anthony
              and Greene, Casey~S.},
    title = {Opportunities and obstacles for deep learning in biology and medicine},
    journal = {Journal of the Royal Society Interface},
    year = {2017},
    volume = {15},
    number = {141},
}


@inproceedings{bernal2017safety++,
  title={Safety++ designing {IoT} and wearable systems for industrial safety through a user centered design approach},
  author={Bernal, Guillermo and Colombo, Sara and Al Ai Baky, Mohammed and Casalegno, Federico},
  booktitle={Proceedings of the 10th International Conference on Pervasive Technologies Related to Assistive Environments},
  pages={163--170},
  year={2017}
}


@article{pakdemirli2019artificial,
  title={Artificial intelligence in radiology: friend or foe? Where are we now and where are we heading?},
  author={Pakdemirli, Emre},
  journal={Acta radiologica open},
  volume={8},
  number={2},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@misc{nhtsa,
    title={Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey},
    howpublished={\href{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115}{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115} (visited 2022-04-20)},
    author={National Highway Transportation Safety Administration's (NHTSA) National Center for Statistics and Analysis},
    publisher={US Department of Transportation},
    year={2015},
}

@article{icoh, title={GLOBAL ESTIMATES OF OCCUPATIONAL  ACCIDENTS AND WORK-RELATED ILLNESSES 2017}, url={http://www.icohweb.org/site/images/news/pdf/Report\%20Global\%20Estimates\%20of\%20Occupational\%20Accidents\%20and\%20Work-related\%20Illnesses\%202017\%20rev1.pdf}, journal={International Commission on Occupational Health`}, publisher={ICOH}, author={ICOH}, year={2017}} 

@misc{OECD,
    title={{OECD} statistics},
    howpublished={\href{https://stats.oecd.org}{https://stats.oecd.org/} (visited 2022-04-20)},
    journal={{OECD} Statistics},
    publisher={International Transport Forum},
    author={The Organisation for Economic Co-operation and Development},
    year={2020},
}

@article{makary2016medical,
  title={Medical error---the third leading cause of death in the {US}},
  author={Makary, Martin A and Daniel, Michael},
  journal={{BMJ}},
  volume={353},
  year={2016},
  publisher={British Medical Journal Publishing Group}
}


@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}
@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@book{aviation_compliance,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{safetyframework,
  title={A framework for safety automation of safety-critical systems operations},
  author={Acharyulu, PV Srinivas and Seetharamaiah, P},
  journal={Safety Science},
  volume={77},
  pages={133--142},
  year={2015},
  publisher={Elsevier}
}

@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}


@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}



@ARTICLE{distributed_attacks,

  author={Aljuhani, Ahamed},

  journal={IEEE Access}, 

  title={Machine Learning Approaches for Combating Distributed Denial of Service Attacks in Modern Networking Environments}, 

  year={2021},

  volume={9},

  number={},

  pages={42236-42264},

  doi={10.1109/ACCESS.2021.3062909}}



@INPROCEEDINGS{intermittent,

  author={Park, Jeman and Nyang, DaeHun and Mohaisen, Aziz},

  booktitle={2018 16th Annual Conference on Privacy, Security and Trust (PST)}, 

  title={Timing is Almost Everything: Realistic Evaluation of the Very Short Intermittent DDoS Attacks}, 

  year={2018},

  volume={},

  number={},

  pages={1-10},

  doi={10.1109/PST.2018.8514210}}





@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}



@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@inproceedings{deka2019adversarial,
  title={Adversarial Impact on Anomaly Detection in Cloud Datacenters},
  author={Deka, Pratyush Kr and Bhuyan, Monowar H and Kadobayashi, Youki and Elmroth, Erik},
  booktitle={2019 IEEE 24th Pacific Rim International Symposium on Dependable Computing (PRDC)},
  pages={188--18809},
  year={2019},
  organization={IEEE}
}

@article{min2020curious,
  title={The curious case of adversarially robust models: More data can help, double descend, or hurt generalization},
  author={Min, Yifei and Chen, Lin and Karbasi, Amin},
  journal={arXiv:2002.11080},
  year={2020}
}

@article{kotyan2022adversarial,
  title={Adversarial robustness assessment: Why in evaluation both L0 and L∞ attacks are necessary},
  author={Kotyan, Shashank and Vargas, Danilo Vasconcellos},
  journal={PLoS ONE},
  volume={17},
  number={4},
  pages={e0265723},
  year={2022},
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press, Cambridge, UK.},
}

@article{blumer1989learnability,
  title={Learnability and the Vapnik-Chervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM},
  volume={36},
  number={4},
  pages={929--965},
  year={1989},
}

@article{aft_models,
  title={Survival analysis part II: multivariate data analysis--an introduction to concepts and methods},
  author={Bradburn, Mike J and Clark, Taane G and Love, Sharon B and Altman, Douglas Graham},
  journal={British journal of cancer},
  volume={89},
  number={3},
  pages={431--436},
  year={2003},
  publisher={Nature Publishing Group}
}

@article{hadj2018continuation,
  title={Continuation of {Nesterov}’s smoothing for regression with structured sparsity in high-dimensional neuroimaging},
  author={Hadj-Selem, Fouad and L{\"o}fstedt, Tommy and Dohmatob, Elvis and Frouin, Vincent and Dubois, Mathieu and Guillemot, Vincent and Duchesnay, Edouard},
  journal={IEEE Transactions on Medical Imaging},
  volume={37},
  number={11},
  pages={2403--2413},
  year={2018},
  publisher={IEEE}
}

@inbook{hoffstein_pipher_silverman_2010,
    title={Complexity Theory and P vs NP},
    booktitle={An introduction to mathematical cryptography},
    publisher={Springer},
    author={Hoffstein, Jeffrey and Pipher, Jill and Silverman, Joseph H.},
    year={2010},
    chapter={4},
    pages={258-262},
}


 @book{IEC61508, edition={2nd}, title={IEC 61508 Safety and Functional Safety}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2010}} 
 
 @book{IEC62034, edition={2nd}, title={IEC 62304 Medical Device Software - Software Life Cycle Processes}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2006}} 

@article{bect_bayesian_2017,
	title = {Bayesian subset simulation},
	volume = {5},
	issn = {2166-2525},
	url = {http://arxiv.org/abs/1601.02557},
	doi = {10.1137/16M1078276},
	abstract = {We consider the problem of estimating a probability of failure α, deﬁned as the volume of the excursion set of a function f : X ⊆ Rd → R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate α when the number of evaluations of f is very limited and α is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to deﬁne the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves signiﬁcant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
	language = {en},
	number = {1},
	urldate = {2020-07-20},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
	month = jan,
	year = {2017},
	keywords = {Statistics - Computation},
	pages = {762--786},
	file = {Bect et al. - 2017 - Bayesian subset simulation.pdf:C\:\\Users\\charlie\\Zotero\\storage\\CFEQS6UF\\Bect et al. - 2017 - Bayesian subset simulation.pdf:application/pdf}
}

@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}

@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}
@Misc{hydra,
  author =       {Omry Yadan},
  title =        {Hydra - A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{orekondy2019knockoff,
  title={Knockoff nets: Stealing functionality of black-box models},
  author={Orekondy, Tribhuvanesh and Schiele, Bernt and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4954--4963},
  year={2019}
}

@inproceedings{li2021membership,
  title={Membership leakage in label-only exposures},
  author={Li, Zheng and Zhang, Yang},
  booktitle={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages={880--895},
  year={2021}
}


@inproceedings{choquette2021label,
  title={Label-only membership inference attacks},
  author={Choquette-Choo, Christopher A and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  booktitle={International conference on machine learning},
  pages={1964--1974},
  year={2021},
  organization={PMLR}
}

@inproceedings{saha2020hidden,
  title={Hidden trigger backdoor attacks},
  author={Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={11957--11965},
  year={2020}
}


@article{rolnick2017power,
  title={The power of deeper networks for expressing natural functions},
  author={Rolnick, David and Tegmark, Max},
  journal={arXiv preprint arXiv:1705.05502},
  year={2017}
}

@article{hochreiter1998vanishing,
  title={The vanishing gradient problem during learning recurrent neural nets and problem solutions},
  author={Hochreiter, Sepp},
  journal={International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume={6},
  number={02},
  pages={107--116},
  year={1998},
  publisher={World Scientific}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@Misc{k8s,
  author =       {Kubernetes},
  title =        {Kubernetes--an open source system for managing containerized applications},
  howpublished = {Github},
  month = {June},
  year =         {2019},
  url =          {https://github.com/kubernetes/kubernetes}
}
@article{meyers_1, title={Safety-Critical Computer Vision: An empirical survey of adversarial evasion attacks and defenses on Computer Vision Systems}, DOI={10.1007/s10462-023-10521-4}, journal={Artificial Intelligence Review}, author={Meyers, Charles and Löfstedt, Tommy and Elmroth, Erik}, year={2023}} 
@Misc{dvc,
  author =       {dvc.org},
  title =        {DVC- Data Version Control},
  howpublished = {Github},
  year =         {2023},
  url =          {https://github.com/iterative/dvc.org}
}

@article{samuel2023computational,
  title={Computational reproducibility of jupyter notebooks from biomedical publications},
  author={Samuel, Sheeba and Mietchen, Daniel},
  journal={arXiv preprint arXiv:2308.07333},
  year={2023}
}

@article{li2019edge,
  title={Edge AI: On-demand accelerating deep neural network inference via edge computing},
  author={Li, En and Zeng, Liekang and Zhou, Zhi and Chen, Xu},
  journal={IEEE Transactions on Wireless Communications},
  volume={19},
  number={1},
  pages={447--457},
  year={2019},
  publisher={IEEE}
}

@article{li2020review,
  title={A review of applications in federated learning},
  author={Li, Li and Fan, Yuxi and Tse, Mike and Lin, Kuo-Yi},
  journal={Computers \& Industrial Engineering},
  volume={149},
  pages={106854},
  year={2020},
  publisher={Elsevier}
}

@article{shamimmachine,
  title={A Machine Learning Model to Protect Privacy Using Federal Learning with Homomorphy Encryption},
  author={Shamim, Rejuwan and Arshad, Md and Pandey, Vinay}
}


@inproceedings{bhagoji2019analyzing,
  title={Analyzing federated learning through an adversarial lens},
  author={Bhagoji, Arjun Nitin and Chakraborty, Supriyo and Mittal, Prateek and Calo, Seraphin},
  booktitle={International Conference on Machine Learning},
  pages={634--643},
  year={2019},
  organization={PMLR}
}

@article{deng2020edge,
  title={Edge intelligence: The confluence of edge computing and artificial intelligence},
  author={Deng, Shuiguang and Zhao, Hailiang and Fang, Weijia and Yin, Jianwei and Dustdar, Schahram and Zomaya, Albert Y},
  journal={IEEE Internet of Things Journal},
  volume={7},
  number={8},
  pages={7457--7469},
  year={2020},
  publisher={IEEE}
}

@article{desislavov2021compute,
  title={Compute and energy consumption trends in deep learning inference},
  author={Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={arXiv:2109.05472},
  year={2021}
}


@article{meyers,
    title = {Safety-critical computer vision: An empirical survey of adversarial evasion attacks and defenses on computer vision systems},
    author = {Meyers, L\"{o}fstedt, Elmroth},
    journal = {Springer Artificial Intelligence Review},
    year = {2023},
}

@article{lifelines,
  doi = {10.21105/joss.01317},
  url = {https://doi.org/10.21105/joss.01317},
  year = {2019},
  publisher = {The Open Journal},
  volume = {4},
  number = {40},
  pages = {1317},
  author = {Cameron Davidson-Pilon},
  title = {lifelines: survival analysis in Python},
  journal = {Journal of Open Source Software}
}

@article{art2018,
    title = {Adversarial Robustness Toolbox v1.2.0},
    author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh~Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian and Edwards, Ben},
    journal = {CoRR},
    volume = {1807.01069},
    year = {2018},
    url = {https://arxiv.org/pdf/1807.01069}
}


@misc{k8s-size, title={Octoverse Projects}, url={https://octoverse.github.com/2018/projects.html}, journal={The State of the Octoverse}, publisher={Github.com}, author={Github}, year={2019}} 

@article{roh2019survey,
  title={A survey on data collection for machine learning: a big data-ai integration perspective},
  author={Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={4},
  pages={1328--1347},
  year={2019},
  publisher={IEEE}
}

@article{sehwag2019towards,
  title={Towards compact and robust deep neural networks},
  author={Sehwag, Vikash and Wang, Shiqi and Mittal, Prateek and Jana, Suman},
  journal={arXiv:1906.06110},
  year={2019}
}


@inproceedings{jakubovitz2018improving,
  title={Improving dnn robustness to adversarial attacks using jacobian regularization},
  author={Jakubovitz, Daniel and Giryes, Raja},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={514--529},
  year={2018}
}

@article{colbrook2021can,
  title={Can stable and accurate neural networks be computed},
  author={Colbrook, Matthew~J. and Antun, Vegard and Hansen, Anders~C.},
  journal={On the barriers of deep learning and Smale’s 18th problem. arXiv},
  volume={2101},
  year={2021}
}

@inproceedings{sinn2019evolutionary,
  title={Evolutionary search for adversarially robust neural networks},
  author={Sinn, Mathieu and Wistuba, M and Buesser, B and Nicolae, MI and Tran, M},
  booktitle={Safe Machine Learning workshop at ICLR},
  year={2019}
}


@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew and Doshi-Velez, Finale},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}
% number={1},


@article{cosentino2019search,
  title={The search for sparse, robust neural networks},
  author={Cosentino, Justin and Zaiter, Federico and Pei, Dan and Zhu, Jun},
  journal={arXiv:1912.02386},
  year={2019}
}

@article{jian2022pruning,
  title={Pruning Adversarially Robust Neural Networks without Adversarial Examples},
  author={Jian, Tong and Wang, Zifeng and Wang, Yanzhi and Dy, Jennifer and Ioannidis, Stratis},
  journal={arXiv:2210.04311},
  year={2022}
}

@inproceedings{lam2004new,
  title={New design-to-test software strategies accelerate time-to-market},
  author={Lam, Hau},
  booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No. 04CH37585)},
  pages={140--143},
  year={2004},
  organization={IEEE}
}

@article{zirger1996effect,
  title={The effect of acceleration techniques on product development time},
  author={Zirger, Billie J and Hartley, Janet L},
  journal={IEEE Transactions on Engineering Management},
  volume={43},
  number={2},
  pages={143--152},
  year={1996},
  publisher={IEEE}
}

@book{ramirez2000resource,
  title={A resource guide on racial profiling data collection systems: Promising practices and lessons learned},
  author={Ramirez, Deborah and McDevitt, Jack and Farrell, Amy},
  year={2000},
  publisher={US Department of Justice}
}

@inproceedings{bloom2017self,
  title={Self-driving cars and data collection: Privacy perceptions of networked autonomous vehicles},
  author={Bloom, Cara and Tan, Joshua and Ramjohn, Javed and Bauer, Lujo},
  booktitle={Symposium on Usable Privacy and Security (SOUPS)},
  year={2017}
}

@article{gichoya2022ai,
  title={AI recognition of patient race in medical imaging: a modelling study},
  author={Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and Burns, John L and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and others},
  journal={The Lancet Digital Health},
  volume={4},
  number={6},
  pages={e406--e414},
  year={2022},
  publisher={Elsevier}
}

@article{evans2001gender,
  title={Gender and age influence on fatality risk from the same physical impact determined using two-car crashes},
  author={Evans, Leonard and Gerrish, Peter H},
  journal={SAE transactions},
  pages={1336--1341},
  year={2001},
  publisher={JSTOR}
}

@article{corsaro1982something,
  title={Something old and something new: The importance of prior ethnography in the collection and analysis of audiovisual data},
  author={Corsaro, William A},
  journal={Sociological Methods \& Research},
  volume={11},
  number={2},
  pages={145--166},
  year={1982},
  publisher={Sage Publications}
}

@article{banks2018driver,
  title={Driver error or designer error: Using the Perceptual Cycle Model to explore the circumstances surrounding the fatal Tesla crash on 7th May 2016},
  author={Banks, Victoria A and Plant, Katherine L and Stanton, Neville A},
  journal={Safety science},
  volume={108},
  pages={278--285},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018},
  organization={PMLR}
}

@article{lu2020gender,
  title={Gender bias in neural natural language processing},
  author={Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  journal={Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday},
  pages={189--202},
  year={2020},
  publisher={Springer}
}

@article{koch2021reduced,
  title={Reduced, reused and recycled: The life of a dataset in machine learning research},
  author={Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob G},
  journal={arXiv preprint arXiv:2112.01716},
  year={2021}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}


@inproceedings{ahn2003captcha,
  title={CAPTCHA: Using hard AI problems for security},
  author={Ahn, Luis von and Blum, Manuel and Hopper, Nicholas J and Langford, John},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={294--311},
  year={2003},
  organization={Springer}
}

@book{aviation,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{ma2020imbalanced,
  title={Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness},
  author={Ma, Xingjun and Jiang, Linxi and Huang, Hanxun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
  journal={arXiv:2006.13726},
  year={2020}
}


@article{chakraborty2018adversarial,
  title={Adversarial attacks and defences: A survey},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={arXiv:1810.00069},
  year={2018}
}

@inproceedings{braking,
  title={Autonomous braking system via deep reinforcement learning},
  author={Hyunmin Chae and Chang Mook Kang and ByeoungDo Kim and Jaekyum Kim and Chung Choo Chung and Jun Won Choi},
  booktitle={{IEEE} 20th International conference on intelligent transportation systems ({ITSC})},
  year={2017},
}

@inproceedings{cintas_detecting_2020,
	address = {Yokohama, Japan},
	title = {Detecting {Adversarial} {Attacks} via {Subset} {Scanning} of {Autoencoder} {Activations} and {Reconstruction} {Error}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/122},
	doi = {10.24963/ijcai.2020/122},
	abstract = {Reliably detecting attacks in a given set of inputs is of high practical relevance because of the vulnerability of neural networks to adversarial examples. These altered inputs create a security risk in applications with real-world consequences, such as self-driving cars, robotics and ﬁnancial services. We propose an unsupervised method for detecting adversarial attacks in inner layers of autoencoder (AE) networks by maximizing a non-parametric measure of anomalous node activations. Previous work in this space has shown AE networks can detect anomalous images by thresholding the reconstruction error produced by the ﬁnal layer. Furthermore, other detection methods rely on data augmentation or specialized training techniques which must be asserted before training time. In contrast, we use subset scanning methods from the anomalous pattern detection domain to enhance detection power without labeled examples of the noise, retraining or data augmentation methods. In addition to an anomalous “score” our proposed method also returns the subset of nodes within the AE network that contributed to that score. This will allow future work to pivot from detection to visualisation and explainability. Our scanning approach shows consistently higher detection power than existing detection methods across several adversarial noise models and a wide range of perturbation strengths.},
	language = {en},
	urldate = {2020-11-03},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Cintas, Celia and Speakman, Skyler and Akinwande, Victor and Ogallo, William and Weldemariam, Komminist and Sridharan, Srihari and McFowland, Edward},
	month = jul,
	year = {2020},
	pages = {876--882},
	file = {Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:C\:\\Users\\charlie\\Zotero\\storage\\RU2HGKV3\\Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:application/pdf}
}

@article{croce_reliable_2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	url = {http://arxiv.org/abs/2003.01690},
	abstract = {The ﬁeld of defense strategies against adversarial attacks has signiﬁcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufﬁcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difﬁcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we ﬁrst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:2003.01690 [cs, stat]},
	author = {Croce, Francesco and Hein, Matthias},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:C\:\\Users\\charlie\\Zotero\\storage\\R7F4DQN8\\Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:application/pdf}
}


@article{paudice_detection_2018,
	title = {Detection of {Adversarial} {Training} {Examples} in {Poisoning} {Attacks} through {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1802.03041},
	abstract = {Machine learning has become an important component for many systems and applications including computer vision, spam ﬁltering, malware and network intrusion detection, among others. Despite the capabilities of machine learning algorithms to extract valuable information from data and produce accurate predictions, it has been shown that these algorithms are vulnerable to attacks. Data poisoning is one of the most relevant security threats against machine learning systems, where attackers can subvert the learning process by injecting malicious samples in the training data. Recent work in adversarial machine learning has shown that the so-called optimal attack strategies can successfully poison linear classiﬁers, degrading the performance of the system dramatically after compromising a small fraction of the training dataset. In this paper we propose a defence mechanism to mitigate the effect of these optimal poisoning attacks based on outlier detection. We show empirically that the adversarial examples generated by these attack strategies are quite different from genuine points, as no detectability constrains are considered to craft the attack. Hence, they can be detected with an appropriate pre-ﬁltering of the training dataset.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1802.03041 [cs, stat]},
	author = {Paudice, Andrea and Muñoz-González, Luis and Gyorgy, Andras and Lupu, Emil C.},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:C\:\\Users\\charlie\\Zotero\\storage\\ABWQPGBP\\Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}


@article{daya_graph-based_2019,
	title = {A {Graph}-{Based} {Machine} {Learning} {Approach} for {Bot} {Detection}},
	url = {http://arxiv.org/abs/1902.08538},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1902.08538 [cs]},
	author = {Daya, Abbas Abou and Salahuddin, Mohammad A. and Limam, Noura and Boutaba, Raouf},
	month = feb,
	year = {2019},
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
	file = {Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:C\:\\Users\\charlie\\Zotero\\storage\\PDN3S6FR\\Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf}
}

@inproceedings{fredrikson_model_2015,
	address = {Denver, Colorado, USA},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	url = {http://dl.acm.org/citation.cfm?doid=2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} - {CCS} '15},
	publisher = {ACM Press},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	year = {2015},
	pages = {1322--1333},
	file = {Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:C\:\\Users\\charlie\\Zotero\\storage\\6A5APDC5\\Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:application/pdf}
}


@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{li_general_2016,
	title = {A {General} {Retraining} {Framework} for {Scalable} {Adversarial} {Classification}},
	url = {http://arxiv.org/abs/1604.02606},
	abstract = {Traditional classiﬁcation algorithms assume that training and test data come from similar distributions. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the ﬁrst systematic and general-purpose retraining framework which can: a) boost robustness of an arbitrary learning algorithm, in the face of b) a broader class of adversarial models than any prior methods. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but are more general and far more scalable. The experiments also conﬁrm that without retraining, our adversarial framework dramatically reduces the effectiveness of learning. In contrast, retraining signiﬁcantly boosts robustness to evasion attacks without signiﬁcantly compromising overall accuracy.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1604.02606 [cs, stat]},
	author = {Li, Bo and Vorobeychik, Yevgeniy and Chen, Xinyun},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:C\:\\Users\\charlie\\Zotero\\storage\\NGCPLE3C\\Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:application/pdf}
}

@article{biggio_evasion_2013,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but eﬀective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classiﬁcation algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit diﬀerent risk levels for the classiﬁer by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples. This gives the classiﬁer designer a better picture of the classiﬁer performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF ﬁles, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {387--402},
	file = {Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:C\:\\Users\\charlie\\Zotero\\storage\\KSVTHWRB\\Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {http://arxiv.org/abs/1802.00420},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1802.00420 [cs]},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:C\:\\Users\\charlie\\Zotero\\storage\\36AD8ZHI\\Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:application/pdf}
}



@INPROCEEDINGS{dohmatob_generalized_2019,
    author = {Dohmatob, Elvis},
    title = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	year = {2019},
    volume = {97},
    series = {PMLR},
}
% 1646-1654, 2019. 

@article{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1805.12152 [cs, stat]},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:C\:\\Users\\charlie\\Zotero\\storage\\2V7GJHIL\\Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}


@article{miller_adversarial_2020,
	title = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against} {Attacks}},
	volume = {108},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9013065/},
	doi = {10.1109/JPROC.2020.2970615},
	language = {en},
	number = {3},
	urldate = {2020-08-12},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	month = mar,
	year = {2020},
	pages = {402--433},
	file = {Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:C\:\\Users\\charlie\\Zotero\\storage\\VV3I4V2C\\Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:application/pdf}
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}



@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}

@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}


@inproceedings{deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}


@article{pixelattack,
  title={Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary},
  author={Kotyan, Shashank and Vasconcellos Vargas, Danilo},
  journal={arXiv e-prints},
  pages={arXiv--1906},
  year={2019}
}

@article{finlayson2018adversarial,
  title={Adversarial Attacks Against Medical Deep Learning Systems},
  author={Finlayson, Samuel G and Chung, Hyung Won and Kohane, Isaac S and Beam, Andrew L},
  journal={arXiv:1804.05296},
  year={2018}
}

@inproceedings{hopskipjump,
  title={{HopSkipJumpAttack}: A query-efficient decision-based attack},
  author={Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
  booktitle={{IEEE} symposium on security and privacy (sp)},
  pages={1277--1294},
  year={2020},
  organization={IEEE}
}

@article{fgm,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv:1412.6572},
  year={2014}
}


@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv:1706.06083},
  year={2017}
}


@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}

@article{wang_security_2019,
	title = {The security of machine learning in an adversarial setting: {A} survey},
	volume = {130},
	issn = {07437315},
	shorttitle = {The security of machine learning in an adversarial setting},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518309183},
	doi = {10.1016/j.jpdc.2019.03.003},
	abstract = {Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models.},
	language = {en},
	urldate = {2020-08-12},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
	month = aug,
	year = {2019},
	pages = {12--23},
	file = {Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:C\:\\Users\\charlie\\Zotero\\storage\\IZAB3YMW\\Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:application/pdf}
}

@article{chakraborty_adversarial_2018,
	title = {Adversarial Attacks and Defences: {A} Survey},
	journal = {arXiv:1810.00069 [cs, stat]},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year = {2018},
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}



@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{biggio_multiple_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiple {Classifier} {Systems} for {Adversarial} {Classification} {Tasks}},
	isbn = {978-3-642-02326-2},
	doi = {10.1007/978-3-642-02326-2_14},
	abstract = {Pattern classification systems are currently used in security applications like intrusion detection in computer networks, spam filtering and biometric identity recognition. These are adversarial classification problems, since the classifier faces an intelligent adversary who adaptively modifies patterns (e.g., spam e-mails) to evade it. In these tasks the goal of a classifier is to attain both a high classification accuracy and a high hardness of evasion, but this issue has not been deeply investigated yet in the literature. We address it under the viewpoint of the choice of the architecture of a multiple classifier system. We propose a measure of the hardness of evasion of a classifier architecture, and give an analytical evaluation and comparison of an individual classifier and a classifier ensemble architecture. We finally report an experimental evaluation on a spam filtering task.},
	language = {en},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer},
	author = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	editor = {Benediktsson, Jón Atli and Kittler, Josef and Roli, Fabio},
	year = {2009},
	pages = {132--141}
}

@article{feature_squeezing,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv:1704.01155},
  year={2017}
}

@inproceedings{label_smoothing,
  title={Adversarial Perturbations of Deep Neural Networks},
  author={D. Warde-Farley and I. Goodfellow},
  editor={T. Hazan , G. Papandreou, D. Tarlow},
  booktitle={Perturbations, Optimization, and Statistics},
  publisher={The MIT Press, Cambridge, Massachusetts},
  year={2017},
}

@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction {API}s},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th {USENIX} Security Symposium ({USENIX} Security 16)},
  pages={601--618},
  year={2016}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}


@article{tramer,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}
@article{papernot,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}


@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@article{adversarialpatch,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv:1712.09665},
  year={2017}
}
@inproceedings{xiao2021improving,
  title={Improving Transferability of Adversarial Patches on Face Recognition With Generative Models},
  author={Xiao, Zihao and Gao, Xianfeng and Fu, Chilin and Dong, Yinpeng and Gao, Wei and Zhang, Xiaolu and Zhou, Jun and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11845--11854},
  year={2021}
}

@article{grigorescu2020survey,
  title={A survey of deep learning techniques for autonomous driving},
  author={Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
  journal={Journal of Field Robotics},
  volume={37},
  number={3},
  pages={362--386},
  year={2020},
  publisher={Wiley Online Library}
}

@book{nelson2010behavior,
  title={Behavior of machine learning algorithms in adversarial environments},
  author={Nelson, Blaine Alan},
  year={2010},
  publisher={University of California, Berkeley}
}


@inproceedings{al2017deep,
  title={Deep learning algorithm for autonomous driving using {GoogLeNet}},
  author={Al-Qizwini, Mohammed and Barjasteh, Iman and Al-Qassab, Hothaifa and Radha, Hayder},
  booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
  pages={89--96},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{gauss_out,
    author={Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
    booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
    title={Certified Robustness to Adversarial Examples with Differential Privacy},
    year={2019},
    pages={656-672},
}
% volume={},
% number={},

@ARTICLE{high_conf,  author={Chen, Li and Xiao, Jun and Zou, Pu and Li, Haifeng},  journal={IEEE Geoscience and Remote Sensing Letters},   title={Lie to Me: A Soft Threshold Defense Method for Adversarial Examples of Remote Sensing Images},   year={2021},  volume={},  number={},  pages={1-5},  doi={10.1109/LGRS.2021.3096244}}

@inproceedings{discretization,
  title={Defending against whitebox adversarial attacks via randomized discretization},
  author={Zhang, Yuchen and Liang, Percy},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={684--693},
  year={2019},
  organization={PMLR}
}


@inproceedings{tuan2010modeling,
  title={Modeling and verification of safety critical systems: A case study on pacemaker},
  author={Tuan, Luu Anh and Zheng, Man Chun and Tho, Quan Thanh},
  booktitle={2010 Fourth International Conference on Secure Software Integration and Reliability Improvement},
  pages={23--32},
  year={2010},
  organization={IEEE}
}

@inproceedings{aviation_software,
  title={Applying lessons from safety-critical systems to security-critical software},
  author={Axelrod, C Warren},
  booktitle={2011 IEEE Long Island Systems, Applications and Technology Conference},
  pages={1--6},
  year={2011},
  organization={IEEE}
}


@book{topologytextbook,
    place={New York},
    title={Introduction to topology},
    publisher={Dover Publications},
    author={Mendelson, Bert},
    year={2012},
}

@inproceedings{gauss_aug,
author = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
title = {Efficient Defenses Against Adversarial Attacks},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140449},
doi = {10.1145/3128572.3140449},
abstract = {Following the recent adoption of deep neural networks (DNN) accross a wide range of
applications, adversarial attacks against these models have proven to be an indisputable
threat. Adversarial samples are crafted with a deliberate intention of undermining
a system. In the case of DNNs, the lack of better understanding of their working has
prevented the development of efficient defenses. In this paper, we propose a new defense
method based on practical observations which is easy to integrate into models and
performs better than state-of-the-art defenses. Our proposed solution is meant to
reinforce the structure of a DNN, making its prediction more stable and less likely
to be fooled by adversarial samples. We conduct an extensive experimental study proving
the efficiency of our method against multiple attacks, comparing it to numerous defenses,
both in white-box and black-box setups. Additionally, the implementation of our method
brings almost no overhead to the training procedure, while maintaining the prediction
performance of the original model on clean samples.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {39–49}, 
numpages = {11},
keywords = {deep neural network, model security, defenses, adversarial learning},
location = {Dallas, Texas, USA},
series = {AISec '17}
}

@article{chambolle2004algorithm,
  title={An algorithm for total variation minimization and applications},
  author={Chambolle, Antonin},
  journal={Journal of Mathematical imaging and vision},
  volume={20},
  number={1},
  pages={89--97},
  year={2004},
  publisher={Springer}
}


@misc{iso26262,
    author={International Standards Organization},
    title={{ISO} 26262-1:2011, Road vehicles --- Functional safety},
    howpublished={\href{https://www.iso.org/standard/43464.html}{https://www.iso.org/standard/43464.html} (visited 2022-04-20)},
    year={2018},
}

@article{monmasson2011fpgas,
  title={FPGAs in industrial control applications},
  author={Monmasson, Eric and Idkhajine, Lahoucine and Cirstea, Marcian N and Bahri, Imene and Tisan, Alin and Naouar, Mohamed Wissem},
  journal={IEEE Transactions on Industrial informatics},
  volume={7},
  number={2},
  pages={224--243},
  year={2011},
  publisher={IEEE}
}


@article{rudin1992nonlinear,
  title={Nonlinear total variation based noise removal algorithms},
  author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  journal={Physica D: nonlinear phenomena},
  volume={60},
  number={1-4},
  pages={259--268},
  year={1992},
  publisher={Elsevier}
}


@article{fukuda1992theory,
  title={Theory and applications of neural networks for industrial control systems},
  author={Fukuda, Toshio and Shibata, Takanori},
  journal={IEEE Transactions on industrial electronics},
  volume={39},
  number={6},
  pages={472--489},
  year={1992},
  publisher={IEEE}
}

@article{reverse_sigmoid,
  author    = {Taesung Lee and
               Benjamin Edwards and
               Ian M. Molloy and
               Dong Su},
  title     = {Defending Against Model Stealing Attacks Using Deceptive Perturbations},
  journal   = {CoRR},
  volume    = {abs/1806.00054},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00054},
  eprinttype = {arXiv},
  eprint    = {1806.00054},
  timestamp = {Wed, 02 Jun 2021 09:13:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00054.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sahiner2019deep,
  title={Deep learning in medical imaging and radiation therapy},
  author={Sahiner, Berkman and Pezeshk, Aria and Hadjiiski, Lubomir M and Wang, Xiaosong and Drukker, Karen and Cha, Kenny H and Summers, Ronald M and Giger, Maryellen L},
  journal={Medical physics},
  volume={46},
  number={1},
  pages={e1--e36},
  year={2019},
  publisher={Wiley Online Library}
}

@book{pearson2005mining,
  title={Mining imperfect data: Dealing with contamination and incomplete records},
  author={Pearson, Ronald K},
  year={2005},
  publisher={SIAM}
}

@ARTICLE{ching2017opportunities,
    author = {Ching, Travers
              and Himmelstein, Daniel~S.
              and Beaulieu-Jones, Brett~K.
              and Kalinin, Alexandr~A.
              and Do, Brian~T.
              and Way, Gregory~P.
              and Ferrero, Enrico
              and Agapow, Paul-Michael
              and Zietz, Michael
              and Hoffman, Michael~M.
              and Xie, Wei
              and Rosen, Gail~L.
              and Lengerich, Benjamin~J.
              and Israeli, Johnny
              and Lanchantin, Jack
              and Woloszynek, Stephen
              and Carpenter, Anne~E.
              and Shrikumar, Avanti
              and Xu, Jinbo
              and Cofer, Evan~M.
              and Lavender, Christopher~A.
              and Turaga, Srinivas~C.
              and Alexandari, Amr~M.
              and Lu, Zhiyong
              and Harris, David~J.
              and DeCaprio, Dave
              and Qi, Yanjun
              and Kundaje, Anshul
              and Peng, Yifan
              and Wiley, Laura~K.
              and Segler, Marwin~H.~S.
              and Boca, Simina~M.
              and Swamidass, S.~ Joshua
              and Huang, Austin
              and Gitter, Anthony
              and Greene, Casey~S.},
    title = {Opportunities and obstacles for deep learning in biology and medicine},
    journal = {Journal of the Royal Society Interface},
    year = {2017},
    volume = {15},
    number = {141},
}


@inproceedings{bernal2017safety++,
  title={Safety++ designing {IoT} and wearable systems for industrial safety through a user centered design approach},
  author={Bernal, Guillermo and Colombo, Sara and Al Ai Baky, Mohammed and Casalegno, Federico},
  booktitle={Proceedings of the 10th International Conference on Pervasive Technologies Related to Assistive Environments},
  pages={163--170},
  year={2017}
}


@article{pakdemirli2019artificial,
  title={Artificial intelligence in radiology: friend or foe? Where are we now and where are we heading?},
  author={Pakdemirli, Emre},
  journal={Acta radiologica open},
  volume={8},
  number={2},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@misc{nhtsa,
    title={Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey},
    howpublished={\href{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115}{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115} (visited 2022-04-20)},
    author={National Highway Transportation Safety Administration's (NHTSA) National Center for Statistics and Analysis},
    publisher={US Department of Transportation},
    year={2015},
}

@article{icoh, title={GLOBAL ESTIMATES OF OCCUPATIONAL  ACCIDENTS AND WORK-RELATED ILLNESSES 2017}, url={http://www.icohweb.org/site/images/news/pdf/Report\%20Global\%20Estimates\%20of\%20Occupational\%20Accidents\%20and\%20Work-related\%20Illnesses\%202017\%20rev1.pdf}, journal={International Commission on Occupational Health`}, publisher={ICOH}, author={ICOH}, year={2017}} 

@misc{OECD,
    title={{OECD} statistics},
    howpublished={\href{https://stats.oecd.org}{https://stats.oecd.org/} (visited 2022-04-20)},
    journal={{OECD} Statistics},
    publisher={International Transport Forum},
    author={The Organisation for Economic Co-operation and Development},
    year={2020},
}

@article{makary2016medical,
  title={Medical error---the third leading cause of death in the {US}},
  author={Makary, Martin A and Daniel, Michael},
  journal={{BMJ}},
  volume={353},
  year={2016},
  publisher={British Medical Journal Publishing Group}
}


@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}
@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@book{aviation_compliance,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{safetyframework,
  title={A framework for safety automation of safety-critical systems operations},
  author={Acharyulu, PV Srinivas and Seetharamaiah, P},
  journal={Safety Science},
  volume={77},
  pages={133--142},
  year={2015},
  publisher={Elsevier}
}

@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}


@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}



@ARTICLE{distributed_attacks,

  author={Aljuhani, Ahamed},

  journal={IEEE Access}, 

  title={Machine Learning Approaches for Combating Distributed Denial of Service Attacks in Modern Networking Environments}, 

  year={2021},

  volume={9},

  number={},

  pages={42236-42264},

  doi={10.1109/ACCESS.2021.3062909}}



@INPROCEEDINGS{intermittent,

  author={Park, Jeman and Nyang, DaeHun and Mohaisen, Aziz},

  booktitle={2018 16th Annual Conference on Privacy, Security and Trust (PST)}, 

  title={Timing is Almost Everything: Realistic Evaluation of the Very Short Intermittent DDoS Attacks}, 

  year={2018},

  volume={},

  number={},

  pages={1-10},

  doi={10.1109/PST.2018.8514210}}





@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}




@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@article{kotyan2022adversarial,
  title={Adversarial robustness assessment: Why in evaluation both L0 and L∞ attacks are necessary},
  author={Kotyan, Shashank and Vargas, Danilo Vasconcellos},
  journal={PLoS ONE},
  volume={17},
  number={4},
  pages={e0265723},
  year={2022},
}

@book{shalev2014understanding,
  title={Understanding machine learning: From theory to algorithms},
  author={Shalev-Shwartz, Shai and Ben-David, Shai},
  year={2014},
  publisher={Cambridge university press, Cambridge, UK.},
}

@article{blumer1989learnability,
  title={Learnability and the Vapnik-Chervonenkis dimension},
  author={Blumer, Anselm and Ehrenfeucht, Andrzej and Haussler, David and Warmuth, Manfred K},
  journal={Journal of the ACM},
  volume={36},
  number={4},
  pages={929--965},
  year={1989},
}

@article{aft_models,
  title={Survival analysis part II: multivariate data analysis--an introduction to concepts and methods},
  author={Bradburn, Mike J and Clark, Taane G and Love, Sharon B and Altman, Douglas Graham},
  journal={British journal of cancer},
  volume={89},
  number={3},
  pages={431--436},
  year={2003},
  publisher={Nature Publishing Group}
}

@article{hadj2018continuation,
  title={Continuation of {Nesterov}’s smoothing for regression with structured sparsity in high-dimensional neuroimaging},
  author={Hadj-Selem, Fouad and L{\"o}fstedt, Tommy and Dohmatob, Elvis and Frouin, Vincent and Dubois, Mathieu and Guillemot, Vincent and Duchesnay, Edouard},
  journal={IEEE Transactions on Medical Imaging},
  volume={37},
  number={11},
  pages={2403--2413},
  year={2018},
  publisher={IEEE}
}

@inbook{hoffstein_pipher_silverman_2010,
    title={Complexity Theory and P vs NP},
    booktitle={An introduction to mathematical cryptography},
    publisher={Springer},
    author={Hoffstein, Jeffrey and Pipher, Jill and Silverman, Joseph H.},
    year={2010},
    chapter={4},
    pages={258-262},
}


 @book{IEC61508, edition={2nd}, title={IEC 61508 Safety and Functional Safety}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2010}} 
 
 @book{IEC62034, edition={2nd}, title={IEC 62304 Medical Device Software - Software Life Cycle Processes}, publisher={International Electrotechnical Commission}, author={International Electrotechnical Commission}, year={2006}} 

@article{bect_bayesian_2017,
	title = {Bayesian subset simulation},
	volume = {5},
	issn = {2166-2525},
	url = {http://arxiv.org/abs/1601.02557},
	doi = {10.1137/16M1078276},
	abstract = {We consider the problem of estimating a probability of failure α, deﬁned as the volume of the excursion set of a function f : X ⊆ Rd → R above a given threshold, under a given probability measure on X. In this article, we combine the popular subset simulation algorithm (Au and Beck, Probab. Eng. Mech. 2001) and our sequential Bayesian approach for the estimation of a probability of failure (Bect, Ginsbourger, Li, Picheny and Vazquez, Stat. Comput. 2012). This makes it possible to estimate α when the number of evaluations of f is very limited and α is very small. The resulting algorithm is called Bayesian subset simulation (BSS). A key idea, as in the subset simulation algorithm, is to estimate the probabilities of a sequence of excursion sets of f above intermediate thresholds, using a sequential Monte Carlo (SMC) approach. A Gaussian process prior on f is used to deﬁne the sequence of densities targeted by the SMC algorithm, and drive the selection of evaluation points of f to estimate the intermediate probabilities. Adaptive procedures are proposed to determine the intermediate thresholds and the number of evaluations to be carried out at each stage of the algorithm. Numerical experiments illustrate that BSS achieves signiﬁcant savings in the number of function evaluations with respect to other Monte Carlo approaches.},
	language = {en},
	number = {1},
	urldate = {2020-07-20},
	journal = {SIAM/ASA Journal on Uncertainty Quantification},
	author = {Bect, Julien and Li, Ling and Vazquez, Emmanuel},
	month = jan,
	year = {2017},
	keywords = {Statistics - Computation},
	pages = {762--786},
	file = {Bect et al. - 2017 - Bayesian subset simulation.pdf:C\:\\Users\\charlie\\Zotero\\storage\\CFEQS6UF\\Bect et al. - 2017 - Bayesian subset simulation.pdf:application/pdf}
}

@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}

@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}
@Misc{hydra,
  author =       {Omry Yadan},
  title =        {Hydra - A framework for elegantly configuring complex applications},
  howpublished = {Github},
  year =         {2019},
  url =          {https://github.com/facebookresearch/hydra}
}

@article{vgg,
  title={Very deep convolutional networks for large-scale image recognition},
  author={Simonyan, Karen and Zisserman, Andrew},
  journal={arXiv preprint arXiv:1409.1556},
  year={2014}
}

@inproceedings{orekondy2019knockoff,
  title={Knockoff nets: Stealing functionality of black-box models},
  author={Orekondy, Tribhuvanesh and Schiele, Bernt and Fritz, Mario},
  booktitle={Proceedings of the IEEE/CVF conference on computer vision and pattern recognition},
  pages={4954--4963},
  year={2019}
}

@inproceedings{li2021membership,
  title={Membership leakage in label-only exposures},
  author={Li, Zheng and Zhang, Yang},
  booktitle={Proceedings of the 2021 ACM SIGSAC Conference on Computer and Communications Security},
  pages={880--895},
  year={2021}
}


@inproceedings{choquette2021label,
  title={Label-only membership inference attacks},
  author={Choquette-Choo, Christopher A and Tramer, Florian and Carlini, Nicholas and Papernot, Nicolas},
  booktitle={International conference on machine learning},
  pages={1964--1974},
  year={2021},
  organization={PMLR}
}

@inproceedings{saha2020hidden,
  title={Hidden trigger backdoor attacks},
  author={Saha, Aniruddha and Subramanya, Akshayvarun and Pirsiavash, Hamed},
  booktitle={Proceedings of the AAAI conference on artificial intelligence},
  volume={34},
  number={07},
  pages={11957--11965},
  year={2020}
}


@article{rolnick2017power,
  title={The power of deeper networks for expressing natural functions},
  author={Rolnick, David and Tegmark, Max},
  journal={arXiv preprint arXiv:1705.05502},
  year={2017}
}

@article{hochreiter1998vanishing,
  title={The vanishing gradient problem during learning recurrent neural nets and problem solutions},
  author={Hochreiter, Sepp},
  journal={International Journal of Uncertainty, Fuzziness and Knowledge-Based Systems},
  volume={6},
  number={02},
  pages={107--116},
  year={1998},
  publisher={World Scientific}
}

@inproceedings{resnet,
  title={Deep residual learning for image recognition},
  author={He, Kaiming and Zhang, Xiangyu and Ren, Shaoqing and Sun, Jian},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={770--778},
  year={2016}
}


@Misc{k8s,
  author =       {Kubernetes},
  title =        {Kubernetes--an open source system for managing containerized applications},
  howpublished = {Github},
  month = {June},
  year =         {2019},
  url =          {https://github.com/kubernetes/kubernetes}
}

@Misc{dvc,
  author =       {dvc.org},
  title =        {DVC- Data Version Control},
  howpublished = {Github},
  year =         {2023},
  url =          {https://github.com/iterative/dvc.org}
}



@article{desislavov2021compute,
  title={Compute and energy consumption trends in deep learning inference},
  author={Desislavov, Radosvet and Mart{\'\i}nez-Plumed, Fernando and Hern{\'a}ndez-Orallo, Jos{\'e}},
  journal={arXiv:2109.05472},
  year={2021}
}


@article{meyers,
    title = {Safety-critical computer vision: An empirical survey of adversarial evasion attacks and defenses on computer vision systems},
    author = {Meyers, L\"{o}fstedt, Elmroth},
    journal = {Springer Artificial Intelligence Review},
    year = {2023},
}

@article{lifelines,
  doi = {10.21105/joss.01317},
  url = {https://doi.org/10.21105/joss.01317},
  year = {2019},
  publisher = {The Open Journal},
  volume = {4},
  number = {40},
  pages = {1317},
  author = {Cameron Davidson-Pilon},
  title = {lifelines: survival analysis in Python},
  journal = {Journal of Open Source Software}
}

@article{art2018,
    title = {Adversarial Robustness Toolbox v1.2.0},
    author = {Nicolae, Maria-Irina and Sinn, Mathieu and Tran, Minh~Ngoc and Buesser, Beat and Rawat, Ambrish and Wistuba, Martin and Zantedeschi, Valentina and Baracaldo, Nathalie and Chen, Bryant and Ludwig, Heiko and Molloy, Ian and Edwards, Ben},
    journal = {CoRR},
    volume = {1807.01069},
    year = {2018},
    url = {https://arxiv.org/pdf/1807.01069}
}


@misc{k8s-size, title={Octoverse Projects}, url={https://octoverse.github.com/2018/projects.html}, journal={The State of the Octoverse}, publisher={Github.com}, author={Github}, year={2019}} 

@article{roh2019survey,
  title={A survey on data collection for machine learning: a big data-ai integration perspective},
  author={Roh, Yuji and Heo, Geon and Whang, Steven Euijong},
  journal={IEEE Transactions on Knowledge and Data Engineering},
  volume={33},
  number={4},
  pages={1328--1347},
  year={2019},
  publisher={IEEE}
}

@article{sehwag2019towards,
  title={Towards compact and robust deep neural networks},
  author={Sehwag, Vikash and Wang, Shiqi and Mittal, Prateek and Jana, Suman},
  journal={arXiv:1906.06110},
  year={2019}
}


@inproceedings{jakubovitz2018improving,
  title={Improving dnn robustness to adversarial attacks using jacobian regularization},
  author={Jakubovitz, Daniel and Giryes, Raja},
  booktitle={Proceedings of the European Conference on Computer Vision (ECCV)},
  pages={514--529},
  year={2018}
}

@article{colbrook2021can,
  title={Can stable and accurate neural networks be computed},
  author={Colbrook, Matthew~J. and Antun, Vegard and Hansen, Anders~C.},
  journal={On the barriers of deep learning and Smale’s 18th problem. arXiv},
  volume={2101},
  year={2021}
}

@inproceedings{sinn2019evolutionary,
  title={Evolutionary search for adversarially robust neural networks},
  author={Sinn, Mathieu and Wistuba, M and Buesser, B and Nicolae, MI and Tran, M},
  booktitle={Safe Machine Learning workshop at ICLR},
  year={2019}
}


@inproceedings{ross2018improving,
  title={Improving the adversarial robustness and interpretability of deep neural networks by regularizing their input gradients},
  author={Ross, Andrew and Doshi-Velez, Finale},
  booktitle={Proceedings of the AAAI Conference on Artificial Intelligence},
  volume={32},
  year={2018}
}
% number={1},


@article{cosentino2019search,
  title={The search for sparse, robust neural networks},
  author={Cosentino, Justin and Zaiter, Federico and Pei, Dan and Zhu, Jun},
  journal={arXiv:1912.02386},
  year={2019}
}

@article{jian2022pruning,
  title={Pruning Adversarially Robust Neural Networks without Adversarial Examples},
  author={Jian, Tong and Wang, Zifeng and Wang, Yanzhi and Dy, Jennifer and Ioannidis, Stratis},
  journal={arXiv:2210.04311},
  year={2022}
}

@inproceedings{lam2004new,
  title={New design-to-test software strategies accelerate time-to-market},
  author={Lam, Hau},
  booktitle={IEEE/CPMT/SEMI 29th International Electronics Manufacturing Technology Symposium (IEEE Cat. No. 04CH37585)},
  pages={140--143},
  year={2004},
  organization={IEEE}
}

@article{zirger1996effect,
  title={The effect of acceleration techniques on product development time},
  author={Zirger, Billie J and Hartley, Janet L},
  journal={IEEE Transactions on Engineering Management},
  volume={43},
  number={2},
  pages={143--152},
  year={1996},
  publisher={IEEE}
}

@book{ramirez2000resource,
  title={A resource guide on racial profiling data collection systems: Promising practices and lessons learned},
  author={Ramirez, Deborah and McDevitt, Jack and Farrell, Amy},
  year={2000},
  publisher={US Department of Justice}
}

@inproceedings{bloom2017self,
  title={Self-driving cars and data collection: Privacy perceptions of networked autonomous vehicles},
  author={Bloom, Cara and Tan, Joshua and Ramjohn, Javed and Bauer, Lujo},
  booktitle={Symposium on Usable Privacy and Security (SOUPS)},
  year={2017}
}

@article{gichoya2022ai,
  title={AI recognition of patient race in medical imaging: a modelling study},
  author={Gichoya, Judy Wawira and Banerjee, Imon and Bhimireddy, Ananth Reddy and Burns, John L and Celi, Leo Anthony and Chen, Li-Ching and Correa, Ramon and Dullerud, Natalie and Ghassemi, Marzyeh and Huang, Shih-Cheng and others},
  journal={The Lancet Digital Health},
  volume={4},
  number={6},
  pages={e406--e414},
  year={2022},
  publisher={Elsevier}
}

@article{evans2001gender,
  title={Gender and age influence on fatality risk from the same physical impact determined using two-car crashes},
  author={Evans, Leonard and Gerrish, Peter H},
  journal={SAE transactions},
  pages={1336--1341},
  year={2001},
  publisher={JSTOR}
}

@article{corsaro1982something,
  title={Something old and something new: The importance of prior ethnography in the collection and analysis of audiovisual data},
  author={Corsaro, William A},
  journal={Sociological Methods \& Research},
  volume={11},
  number={2},
  pages={145--166},
  year={1982},
  publisher={Sage Publications}
}

@article{banks2018driver,
  title={Driver error or designer error: Using the Perceptual Cycle Model to explore the circumstances surrounding the fatal Tesla crash on 7th May 2016},
  author={Banks, Victoria A and Plant, Katherine L and Stanton, Neville A},
  journal={Safety science},
  volume={108},
  pages={278--285},
  year={2018},
  publisher={Elsevier}
}


@inproceedings{buolamwini2018gender,
  title={Gender shades: Intersectional accuracy disparities in commercial gender classification},
  author={Buolamwini, Joy and Gebru, Timnit},
  booktitle={Conference on fairness, accountability and transparency},
  pages={77--91},
  year={2018},
  organization={PMLR}
}

@article{lu2020gender,
  title={Gender bias in neural natural language processing},
  author={Lu, Kaiji and Mardziel, Piotr and Wu, Fangjing and Amancharla, Preetam and Datta, Anupam},
  journal={Logic, Language, and Security: Essays Dedicated to Andre Scedrov on the Occasion of His 65th Birthday},
  pages={189--202},
  year={2020},
  publisher={Springer}
}

@article{koch2021reduced,
  title={Reduced, reused and recycled: The life of a dataset in machine learning research},
  author={Koch, Bernard and Denton, Emily and Hanna, Alex and Foster, Jacob G},
  journal={arXiv preprint arXiv:2112.01716},
  year={2021}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@article{vapnik1994measuring,
  title={Measuring the VC-dimension of a learning machine},
  author={Vapnik, Vladimir and Levin, Esther and Le Cun, Yann},
  journal={Neural computation},
  volume={6},
  number={5},
  pages={851--876},
  year={1994},
  publisher={MIT Press}
}


@inproceedings{ahn2003captcha,
  title={CAPTCHA: Using hard AI problems for security},
  author={Ahn, Luis von and Blum, Manuel and Hopper, Nicholas J and Langford, John},
  booktitle={International conference on the theory and applications of cryptographic techniques},
  pages={294--311},
  year={2003},
  organization={Springer}
}

@book{aviation,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{ma2020imbalanced,
  title={Imbalanced Gradients: A Subtle Cause of Overestimated Adversarial Robustness},
  author={Ma, Xingjun and Jiang, Linxi and Huang, Hanxun and Weng, Zejia and Bailey, James and Jiang, Yu-Gang},
  journal={arXiv:2006.13726},
  year={2020}
}


@article{chakraborty2018adversarial,
  title={Adversarial attacks and defences: A survey},
  author={Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
  journal={arXiv:1810.00069},
  year={2018}
}

@inproceedings{braking,
  title={Autonomous braking system via deep reinforcement learning},
  author={Hyunmin Chae and Chang Mook Kang and ByeoungDo Kim and Jaekyum Kim and Chung Choo Chung and Jun Won Choi},
  booktitle={{IEEE} 20th International conference on intelligent transportation systems ({ITSC})},
  year={2017},
}

@inproceedings{cintas_detecting_2020,
	address = {Yokohama, Japan},
	title = {Detecting {Adversarial} {Attacks} via {Subset} {Scanning} of {Autoencoder} {Activations} and {Reconstruction} {Error}},
	isbn = {978-0-9992411-6-5},
	url = {https://www.ijcai.org/proceedings/2020/122},
	doi = {10.24963/ijcai.2020/122},
	abstract = {Reliably detecting attacks in a given set of inputs is of high practical relevance because of the vulnerability of neural networks to adversarial examples. These altered inputs create a security risk in applications with real-world consequences, such as self-driving cars, robotics and ﬁnancial services. We propose an unsupervised method for detecting adversarial attacks in inner layers of autoencoder (AE) networks by maximizing a non-parametric measure of anomalous node activations. Previous work in this space has shown AE networks can detect anomalous images by thresholding the reconstruction error produced by the ﬁnal layer. Furthermore, other detection methods rely on data augmentation or specialized training techniques which must be asserted before training time. In contrast, we use subset scanning methods from the anomalous pattern detection domain to enhance detection power without labeled examples of the noise, retraining or data augmentation methods. In addition to an anomalous “score” our proposed method also returns the subset of nodes within the AE network that contributed to that score. This will allow future work to pivot from detection to visualisation and explainability. Our scanning approach shows consistently higher detection power than existing detection methods across several adversarial noise models and a wide range of perturbation strengths.},
	language = {en},
	urldate = {2020-11-03},
	booktitle = {Proceedings of the {Twenty}-{Ninth} {International} {Joint} {Conference} on {Artificial} {Intelligence}},
	author = {Cintas, Celia and Speakman, Skyler and Akinwande, Victor and Ogallo, William and Weldemariam, Komminist and Sridharan, Srihari and McFowland, Edward},
	month = jul,
	year = {2020},
	pages = {876--882},
	file = {Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:C\:\\Users\\charlie\\Zotero\\storage\\RU2HGKV3\\Cintas et al. - 2020 - Detecting Adversarial Attacks via Subset Scanning .pdf:application/pdf}
}

@article{croce_reliable_2020,
	title = {Reliable evaluation of adversarial robustness with an ensemble of diverse parameter-free attacks},
	url = {http://arxiv.org/abs/2003.01690},
	abstract = {The ﬁeld of defense strategies against adversarial attacks has signiﬁcantly grown over the last years, but progress is hampered as the evaluation of adversarial defenses is often insufﬁcient and thus gives a wrong impression of robustness. Many promising defenses could be broken later on, making it difﬁcult to identify the state-of-the-art. Frequent pitfalls in the evaluation are improper tuning of hyperparameters of the attacks, gradient obfuscation or masking. In this paper we ﬁrst propose two extensions of the PGD-attack overcoming failures due to suboptimal step size and problems of the objective function. We then combine our novel attacks with two complementary existing ones to form a parameter-free, computationally affordable and user-independent ensemble of attacks to test adversarial robustness. We apply our ensemble to over 50 models from papers published at recent top machine learning and computer vision venues. In all except one of the cases we achieve lower robust test accuracy than reported in these papers, often by more than 10\%, identifying several broken defenses.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:2003.01690 [cs, stat]},
	author = {Croce, Francesco and Hein, Matthias},
	month = aug,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Computer Vision and Pattern Recognition},
	file = {Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:C\:\\Users\\charlie\\Zotero\\storage\\R7F4DQN8\\Croce and Hein - 2020 - Reliable evaluation of adversarial robustness with.pdf:application/pdf}
}


@article{paudice_detection_2018,
	title = {Detection of {Adversarial} {Training} {Examples} in {Poisoning} {Attacks} through {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1802.03041},
	abstract = {Machine learning has become an important component for many systems and applications including computer vision, spam ﬁltering, malware and network intrusion detection, among others. Despite the capabilities of machine learning algorithms to extract valuable information from data and produce accurate predictions, it has been shown that these algorithms are vulnerable to attacks. Data poisoning is one of the most relevant security threats against machine learning systems, where attackers can subvert the learning process by injecting malicious samples in the training data. Recent work in adversarial machine learning has shown that the so-called optimal attack strategies can successfully poison linear classiﬁers, degrading the performance of the system dramatically after compromising a small fraction of the training dataset. In this paper we propose a defence mechanism to mitigate the effect of these optimal poisoning attacks based on outlier detection. We show empirically that the adversarial examples generated by these attack strategies are quite different from genuine points, as no detectability constrains are considered to craft the attack. Hence, they can be detected with an appropriate pre-ﬁltering of the training dataset.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1802.03041 [cs, stat]},
	author = {Paudice, Andrea and Muñoz-González, Luis and Gyorgy, Andras and Lupu, Emil C.},
	month = feb,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:C\:\\Users\\charlie\\Zotero\\storage\\ABWQPGBP\\Paudice et al. - 2018 - Detection of Adversarial Training Examples in Pois.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}


@article{daya_graph-based_2019,
	title = {A {Graph}-{Based} {Machine} {Learning} {Approach} for {Bot} {Detection}},
	url = {http://arxiv.org/abs/1902.08538},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1902.08538 [cs]},
	author = {Daya, Abbas Abou and Salahuddin, Mohammad A. and Limam, Noura and Boutaba, Raouf},
	month = feb,
	year = {2019},
}

@article{carlini_towards_2017,
	title = {Towards {Evaluating} the {Robustness} of {Neural} {Networks}},
	url = {http://arxiv.org/abs/1608.04644},
	abstract = {Neural networks provide state-of-the-art results for most machine learning tasks. Unfortunately, neural networks are vulnerable to adversarial examples: given an input x and any target classiﬁcation t, it is possible to ﬁnd a new input x that is similar to x but classiﬁed as t. This makes it difﬁcult to apply neural networks in security-critical areas. Defensive distillation is a recently proposed approach that can take an arbitrary neural network, and increase its robustness, reducing the success rate of current attacks’ ability to ﬁnd adversarial examples from 95\% to 0.5\%.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1608.04644 [cs]},
	author = {Carlini, Nicholas and Wagner, David},
	month = mar,
	year = {2017},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Computer Vision and Pattern Recognition},
	file = {Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:C\:\\Users\\charlie\\Zotero\\storage\\PDN3S6FR\\Carlini and Wagner - 2017 - Towards Evaluating the Robustness of Neural Networ.pdf:application/pdf}
}

@inproceedings{fredrikson_model_2015,
	address = {Denver, Colorado, USA},
	title = {Model {Inversion} {Attacks} that {Exploit} {Confidence} {Information} and {Basic} {Countermeasures}},
	isbn = {978-1-4503-3832-5},
	url = {http://dl.acm.org/citation.cfm?doid=2810103.2813677},
	doi = {10.1145/2810103.2813677},
	abstract = {Machine-learning (ML) algorithms are increasingly utilized in privacy-sensitive applications such as predicting lifestyle choices, making medical diagnoses, and facial recognition. In a model inversion attack, recently introduced in a case study of linear classiﬁers in personalized medicine by Fredrikson et al. [13], adversarial access to an ML model is abused to learn sensitive genomic information about individuals. Whether model inversion attacks apply to settings outside theirs, however, is unknown.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 22nd {ACM} {SIGSAC} {Conference} on {Computer} and {Communications} {Security} - {CCS} '15},
	publisher = {ACM Press},
	author = {Fredrikson, Matt and Jha, Somesh and Ristenpart, Thomas},
	year = {2015},
	pages = {1322--1333},
	file = {Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:C\:\\Users\\charlie\\Zotero\\storage\\6A5APDC5\\Fredrikson et al. - 2015 - Model Inversion Attacks that Exploit Confidence In.pdf:application/pdf}
}


@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{li_general_2016,
	title = {A {General} {Retraining} {Framework} for {Scalable} {Adversarial} {Classification}},
	url = {http://arxiv.org/abs/1604.02606},
	abstract = {Traditional classiﬁcation algorithms assume that training and test data come from similar distributions. This assumption is violated in adversarial settings, where malicious actors modify instances to evade detection. A number of custom methods have been developed for both adversarial evasion attacks and robust learning. We propose the ﬁrst systematic and general-purpose retraining framework which can: a) boost robustness of an arbitrary learning algorithm, in the face of b) a broader class of adversarial models than any prior methods. We show that, under natural conditions, the retraining framework minimizes an upper bound on optimal adversarial risk, and show how to extend this result to account for approximations of evasion attacks. Extensive experimental evaluation demonstrates that our retraining methods are nearly indistinguishable from state-of-the-art algorithms for optimizing adversarial risk, but are more general and far more scalable. The experiments also conﬁrm that without retraining, our adversarial framework dramatically reduces the effectiveness of learning. In contrast, retraining signiﬁcantly boosts robustness to evasion attacks without signiﬁcantly compromising overall accuracy.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1604.02606 [cs, stat]},
	author = {Li, Bo and Vorobeychik, Yevgeniy and Chen, Xinyun},
	month = nov,
	year = {2016},
	keywords = {Computer Science - Computer Science and Game Theory, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:C\:\\Users\\charlie\\Zotero\\storage\\NGCPLE3C\\Li et al. - 2016 - A General Retraining Framework for Scalable Advers.pdf:application/pdf}
}

@article{biggio_evasion_2013,
	title = {Evasion {Attacks} against {Machine} {Learning} at {Test} {Time}},
	volume = {7908},
	url = {http://arxiv.org/abs/1708.06131},
	doi = {10.1007/978-3-642-40994-3_25},
	abstract = {In security-sensitive applications, the success of machine learning depends on a thorough vetting of their resistance to adversarial data. In one pertinent, well-motivated attack scenario, an adversary may attempt to evade a deployed system at test time by carefully manipulating attack samples. In this work, we present a simple but eﬀective gradientbased approach that can be exploited to systematically assess the security of several, widely-used classiﬁcation algorithms against evasion attacks. Following a recently proposed framework for security evaluation, we simulate attack scenarios that exhibit diﬀerent risk levels for the classiﬁer by increasing the attacker’s knowledge of the system and her ability to manipulate attack samples. This gives the classiﬁer designer a better picture of the classiﬁer performance under evasion attacks, and allows him to perform a more informed model selection (or parameter setting). We evaluate our approach on the relevant security task of malware detection in PDF ﬁles, and show that such systems can be easily evaded. We also sketch some countermeasures suggested by our analysis.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1708.06131 [cs]},
	author = {Biggio, Battista and Corona, Igino and Maiorca, Davide and Nelson, Blaine and Srndic, Nedim and Laskov, Pavel and Giacinto, Giorgio and Roli, Fabio},
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	pages = {387--402},
	file = {Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:C\:\\Users\\charlie\\Zotero\\storage\\KSVTHWRB\\Biggio et al. - 2013 - Evasion Attacks against Machine Learning at Test T.pdf:application/pdf}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{athalye_obfuscated_2018,
	title = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}: {Circumventing} {Defenses} to {Adversarial} {Examples}},
	shorttitle = {Obfuscated {Gradients} {Give} a {False} {Sense} of {Security}},
	url = {http://arxiv.org/abs/1802.00420},
	abstract = {We identify obfuscated gradients, a kind of gradient masking, as a phenomenon that leads to a false sense of security in defenses against adversarial examples. While defenses that cause obfuscated gradients appear to defeat iterative optimizationbased attacks, we ﬁnd defenses relying on this effect can be circumvented. We describe characteristic behaviors of defenses exhibiting the effect, and for each of the three types of obfuscated gradients we discover, we develop attack techniques to overcome it. In a case study, examining noncertiﬁed white-box-secure defenses at ICLR 2018, we ﬁnd obfuscated gradients are a common occurrence, with 7 of 9 defenses relying on obfuscated gradients. Our new attacks successfully circumvent 6 completely, and 1 partially, in the original threat model each paper considers.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1802.00420 [cs]},
	author = {Athalye, Anish and Carlini, Nicholas and Wagner, David},
	month = jul,
	year = {2018},
	keywords = {Computer Science - Artificial Intelligence, Computer Science - Cryptography and Security, Computer Science - Machine Learning},
	file = {Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:C\:\\Users\\charlie\\Zotero\\storage\\36AD8ZHI\\Athalye et al. - 2018 - Obfuscated Gradients Give a False Sense of Securit.pdf:application/pdf}
}



@INPROCEEDINGS{dohmatob_generalized_2019,
    author = {Dohmatob, Elvis},
    title = {Generalized {No} {Free} {Lunch} {Theorem} for {Adversarial} {Robustness}},
    booktitle = {Proceedings of the 36th International Conference on Machine Learning},
	year = {2019},
    volume = {97},
    series = {PMLR},
}
% 1646-1654, 2019. 

@article{tsipras_robustness_2019,
	title = {Robustness {May} {Be} at {Odds} with {Accuracy}},
	url = {http://arxiv.org/abs/1805.12152},
	abstract = {We show that there may exist an inherent tension between the goal of adversarial robustness and that of standard generalization. Speciﬁcally, training robust models may not only be more resource-consuming, but also lead to a reduction of standard accuracy. We demonstrate that this trade-off between the standard accuracy of a model and its robustness to adversarial perturbations provably exists in a fairly simple and natural setting. These ﬁndings also corroborate a similar phenomenon observed empirically in more complex settings. Further, we argue that this phenomenon is a consequence of robust classiﬁers learning fundamentally different feature representations than standard classiﬁers. These differences, in particular, seem to result in unexpected beneﬁts: the representations learned by robust models tend to align better with salient data characteristics and human perception.},
	language = {en},
	urldate = {2020-10-01},
	journal = {arXiv:1805.12152 [cs, stat]},
	author = {Tsipras, Dimitris and Santurkar, Shibani and Engstrom, Logan and Turner, Alexander and Madry, Aleksander},
	month = sep,
	year = {2019},
	keywords = {Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, Computer Science - Neural and Evolutionary Computing, Statistics - Machine Learning},
	file = {Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:C\:\\Users\\charlie\\Zotero\\storage\\2V7GJHIL\\Tsipras et al. - 2019 - Robustness May Be at Odds with Accuracy.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}


@article{miller_adversarial_2020,
	title = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}: {A} {Comprehensive} {Review} of {Defenses} {Against} {Attacks}},
	volume = {108},
	issn = {0018-9219, 1558-2256},
	shorttitle = {Adversarial {Learning} {Targeting} {Deep} {Neural} {Network} {Classification}},
	url = {https://ieeexplore.ieee.org/document/9013065/},
	doi = {10.1109/JPROC.2020.2970615},
	language = {en},
	number = {3},
	urldate = {2020-08-12},
	journal = {Proceedings of the IEEE},
	author = {Miller, David J. and Xiang, Zhen and Kesidis, George},
	month = mar,
	year = {2020},
	pages = {402--433},
	file = {Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:C\:\\Users\\charlie\\Zotero\\storage\\VV3I4V2C\\Miller et al. - 2020 - Adversarial Learning Targeting Deep Neural Network.pdf:application/pdf}
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}



@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}

@article{biggio_poisoning_2013,
	title = {Poisoning {Attacks} against {Support} {Vector} {Machines}},
	url = {http://arxiv.org/abs/1206.6389},
	abstract = {We investigate a family of poisoning attacks against Support Vector Machines (SVM). Such attacks inject specially crafted training data that increases the SVM's test error. Central to the motivation for these attacks is the fact that most learning algorithms assume that their training data comes from a natural or well-behaved distribution. However, this assumption does not generally hold in security-sensitive settings. As we demonstrate, an intelligent adversary can, to some extent, predict the change of the SVM's decision function due to malicious input and use this ability to construct malicious data. The proposed attack uses a gradient ascent strategy in which the gradient is computed based on properties of the SVM's optimal solution. This method can be kernelized and enables the attack to be constructed in the input space even for non-linear kernels. We experimentally demonstrate that our gradient ascent procedure reliably identifies good local maxima of the non-convex validation error surface, which significantly increases the classifier's test error.},
	urldate = {2020-11-02},
	journal = {arXiv:1206.6389 [cs, stat]},
	author = {Biggio, Battista and Nelson, Blaine and Laskov, Pavel},
	month = mar,
	year = {2013},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\MXH3JBV6\\Biggio et al. - 2013 - Poisoning Attacks against Support Vector Machines.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\P4HDZ8AU\\1206.html:text/html}
}

@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}


@inproceedings{deepfool,
  title={Deepfool: a simple and accurate method to fool deep neural networks},
  author={Moosavi-Dezfooli, Seyed-Mohsen and Fawzi, Alhussein and Frossard, Pascal},
  booktitle={Proceedings of the IEEE conference on computer vision and pattern recognition},
  pages={2574--2582},
  year={2016}
}


@article{pixelattack,
  title={Adversarial Robustness Assessment: Why both $L_0$ and $L_\infty$ Attacks Are Necessary},
  author={Kotyan, Shashank and Vasconcellos Vargas, Danilo},
  journal={arXiv e-prints},
  pages={arXiv--1906},
  year={2019}
}

@article{finlayson2018adversarial,
  title={Adversarial Attacks Against Medical Deep Learning Systems},
  author={Finlayson, Samuel G and Chung, Hyung Won and Kohane, Isaac S and Beam, Andrew L},
  journal={arXiv:1804.05296},
  year={2018}
}

@inproceedings{hopskipjump,
  title={{HopSkipJumpAttack}: A query-efficient decision-based attack},
  author={Chen, Jianbo and Jordan, Michael I and Wainwright, Martin J},
  booktitle={{IEEE} symposium on security and privacy (sp)},
  pages={1277--1294},
  year={2020},
  organization={IEEE}
}

@article{fgm,
  title={Explaining and harnessing adversarial examples},
  author={Goodfellow, Ian J and Shlens, Jonathon and Szegedy, Christian},
  journal={arXiv:1412.6572},
  year={2014}
}


@article{madry2017towards,
  title={Towards deep learning models resistant to adversarial attacks},
  author={Madry, Aleksander and Makelov, Aleksandar and Schmidt, Ludwig and Tsipras, Dimitris and Vladu, Adrian},
  journal={arXiv:1706.06083},
  year={2017}
}


@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}


@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}

@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}

@article{wang_security_2019,
	title = {The security of machine learning in an adversarial setting: {A} survey},
	volume = {130},
	issn = {07437315},
	shorttitle = {The security of machine learning in an adversarial setting},
	url = {https://linkinghub.elsevier.com/retrieve/pii/S0743731518309183},
	doi = {10.1016/j.jpdc.2019.03.003},
	abstract = {Machine learning (ML) methods have demonstrated impressive performance in many application fields such as autopilot, facial recognition, and spam detection. Traditionally, ML models are trained and deployed in a benign setting, in which the testing and training data have identical statistical characteristics. However, this assumption usually does not hold in the sense that the ML model is designed in an adversarial setting, where some statistical properties of the data can be tampered with by a capable adversary. Specifically, it has been observed that adversarial examples (also known as adversarial input perambulations) elaborately crafted during training/test phases can seriously undermine the ML performance. The susceptibility of ML models in adversarial settings and the corresponding countermeasures have been studied by many researchers in both academic and industrial communities. In this work, we present a comprehensive overview of the investigation of the security properties of ML algorithms under adversarial settings. First, we analyze the ML security model to develop a blueprint for this interdisciplinary research area. Then, we review adversarial attack methods and discuss the defense strategies against them. Finally, relying upon the reviewed work, we provide prospective relevant future works for designing more secure ML models.},
	language = {en},
	urldate = {2020-08-12},
	journal = {Journal of Parallel and Distributed Computing},
	author = {Wang, Xianmin and Li, Jing and Kuang, Xiaohui and Tan, Yu-an and Li, Jin},
	month = aug,
	year = {2019},
	pages = {12--23},
	file = {Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:C\:\\Users\\charlie\\Zotero\\storage\\IZAB3YMW\\Wang et al. - 2019 - The security of machine learning in an adversarial.pdf:application/pdf}
}

@article{chakraborty_adversarial_2018,
	title = {Adversarial Attacks and Defences: {A} Survey},
	journal = {arXiv:1810.00069 [cs, stat]},
	author = {Chakraborty, Anirban and Alam, Manaar and Dey, Vishal and Chattopadhyay, Anupam and Mukhopadhyay, Debdeep},
	year = {2018},
}

@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}



@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{biggio_multiple_2009,
	address = {Berlin, Heidelberg},
	series = {Lecture {Notes} in {Computer} {Science}},
	title = {Multiple {Classifier} {Systems} for {Adversarial} {Classification} {Tasks}},
	isbn = {978-3-642-02326-2},
	doi = {10.1007/978-3-642-02326-2_14},
	abstract = {Pattern classification systems are currently used in security applications like intrusion detection in computer networks, spam filtering and biometric identity recognition. These are adversarial classification problems, since the classifier faces an intelligent adversary who adaptively modifies patterns (e.g., spam e-mails) to evade it. In these tasks the goal of a classifier is to attain both a high classification accuracy and a high hardness of evasion, but this issue has not been deeply investigated yet in the literature. We address it under the viewpoint of the choice of the architecture of a multiple classifier system. We propose a measure of the hardness of evasion of a classifier architecture, and give an analytical evaluation and comparison of an individual classifier and a classifier ensemble architecture. We finally report an experimental evaluation on a spam filtering task.},
	language = {en},
	booktitle = {Multiple {Classifier} {Systems}},
	publisher = {Springer},
	author = {Biggio, Battista and Fumera, Giorgio and Roli, Fabio},
	editor = {Benediktsson, Jón Atli and Kittler, Josef and Roli, Fabio},
	year = {2009},
	pages = {132--141}
}

@article{feature_squeezing,
  title={Feature squeezing: Detecting adversarial examples in deep neural networks},
  author={Xu, Weilin and Evans, David and Qi, Yanjun},
  journal={arXiv:1704.01155},
  year={2017}
}

@inproceedings{label_smoothing,
  title={Adversarial Perturbations of Deep Neural Networks},
  author={D. Warde-Farley and I. Goodfellow},
  editor={T. Hazan , G. Papandreou, D. Tarlow},
  booktitle={Perturbations, Optimization, and Statistics},
  publisher={The MIT Press, Cambridge, Massachusetts},
  year={2017},
}

@inproceedings{tramer2016stealing,
  title={Stealing machine learning models via prediction {API}s},
  author={Tram{\`e}r, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
  booktitle={25th {USENIX} Security Symposium ({USENIX} Security 16)},
  pages={601--618},
  year={2016}
}

@inproceedings{shokri2017membership,
  title={Membership inference attacks against machine learning models},
  author={Shokri, Reza and Stronati, Marco and Song, Congzheng and Shmatikov, Vitaly},
  booktitle={2017 IEEE Symposium on Security and Privacy (SP)},
  pages={3--18},
  year={2017},
  organization={IEEE}
}


@article{tramer,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}
@article{papernot,
  title={The space of transferable adversarial examples},
  author={Tram{\`e}r, Florian and Papernot, Nicolas and Goodfellow, Ian and Boneh, Dan and McDaniel, Patrick},
  journal={arXiv:1704.03453},
  year={2017}
}


@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@article{adversarialpatch,
  title={Adversarial patch},
  author={Brown, Tom B and Man{\'e}, Dandelion and Roy, Aurko and Abadi, Mart{\'\i}n and Gilmer, Justin},
  journal={arXiv:1712.09665},
  year={2017}
}
@inproceedings{xiao2021improving,
  title={Improving Transferability of Adversarial Patches on Face Recognition With Generative Models},
  author={Xiao, Zihao and Gao, Xianfeng and Fu, Chilin and Dong, Yinpeng and Gao, Wei and Zhang, Xiaolu and Zhou, Jun and Zhu, Jun},
  booktitle={Proceedings of the IEEE/CVF Conference on Computer Vision and Pattern Recognition},
  pages={11845--11854},
  year={2021}
}

@article{grigorescu2020survey,
  title={A survey of deep learning techniques for autonomous driving},
  author={Grigorescu, Sorin and Trasnea, Bogdan and Cocias, Tiberiu and Macesanu, Gigel},
  journal={Journal of Field Robotics},
  volume={37},
  number={3},
  pages={362--386},
  year={2020},
  publisher={Wiley Online Library}
}

@book{nelson2010behavior,
  title={Behavior of machine learning algorithms in adversarial environments},
  author={Nelson, Blaine Alan},
  year={2010},
  publisher={University of California, Berkeley}
}


@inproceedings{al2017deep,
  title={Deep learning algorithm for autonomous driving using {GoogLeNet}},
  author={Al-Qizwini, Mohammed and Barjasteh, Iman and Al-Qassab, Hothaifa and Radha, Hayder},
  booktitle={2017 IEEE Intelligent Vehicles Symposium (IV)},
  pages={89--96},
  year={2017},
  organization={IEEE}
}

@INPROCEEDINGS{gauss_out,
    author={Lecuyer, Mathias and Atlidakis, Vaggelis and Geambasu, Roxana and Hsu, Daniel and Jana, Suman},
    booktitle={2019 IEEE Symposium on Security and Privacy (SP)},
    title={Certified Robustness to Adversarial Examples with Differential Privacy},
    year={2019},
    pages={656-672},
}
% volume={},
% number={},

@ARTICLE{high_conf,  author={Chen, Li and Xiao, Jun and Zou, Pu and Li, Haifeng},  journal={IEEE Geoscience and Remote Sensing Letters},   title={Lie to Me: A Soft Threshold Defense Method for Adversarial Examples of Remote Sensing Images},   year={2021},  volume={},  number={},  pages={1-5},  doi={10.1109/LGRS.2021.3096244}}

@inproceedings{discretization,
  title={Defending against whitebox adversarial attacks via randomized discretization},
  author={Zhang, Yuchen and Liang, Percy},
  booktitle={The 22nd International Conference on Artificial Intelligence and Statistics},
  pages={684--693},
  year={2019},
  organization={PMLR}
}


@inproceedings{tuan2010modeling,
  title={Modeling and verification of safety critical systems: A case study on pacemaker},
  author={Tuan, Luu Anh and Zheng, Man Chun and Tho, Quan Thanh},
  booktitle={2010 Fourth International Conference on Secure Software Integration and Reliability Improvement},
  pages={23--32},
  year={2010},
  organization={IEEE}
}

@inproceedings{aviation_software,
  title={Applying lessons from safety-critical systems to security-critical software},
  author={Axelrod, C Warren},
  booktitle={2011 IEEE Long Island Systems, Applications and Technology Conference},
  pages={1--6},
  year={2011},
  organization={IEEE}
}


@book{topologytextbook,
    place={New York},
    title={Introduction to topology},
    publisher={Dover Publications},
    author={Mendelson, Bert},
    year={2012},
}

@inproceedings{gauss_aug,
author = {Zantedeschi, Valentina and Nicolae, Maria-Irina and Rawat, Ambrish},
title = {Efficient Defenses Against Adversarial Attacks},
year = {2017},
isbn = {9781450352024},
publisher = {Association for Computing Machinery},
address = {New York, NY, USA},
url = {https://doi.org/10.1145/3128572.3140449},
doi = {10.1145/3128572.3140449},
abstract = {Following the recent adoption of deep neural networks (DNN) accross a wide range of
applications, adversarial attacks against these models have proven to be an indisputable
threat. Adversarial samples are crafted with a deliberate intention of undermining
a system. In the case of DNNs, the lack of better understanding of their working has
prevented the development of efficient defenses. In this paper, we propose a new defense
method based on practical observations which is easy to integrate into models and
performs better than state-of-the-art defenses. Our proposed solution is meant to
reinforce the structure of a DNN, making its prediction more stable and less likely
to be fooled by adversarial samples. We conduct an extensive experimental study proving
the efficiency of our method against multiple attacks, comparing it to numerous defenses,
both in white-box and black-box setups. Additionally, the implementation of our method
brings almost no overhead to the training procedure, while maintaining the prediction
performance of the original model on clean samples.},
booktitle = {Proceedings of the 10th ACM Workshop on Artificial Intelligence and Security},
pages = {39–49}, 
numpages = {11},
keywords = {deep neural network, model security, defenses, adversarial learning},
location = {Dallas, Texas, USA},
series = {AISec '17}
}

@article{chambolle2004algorithm,
  title={An algorithm for total variation minimization and applications},
  author={Chambolle, Antonin},
  journal={Journal of Mathematical imaging and vision},
  volume={20},
  number={1},
  pages={89--97},
  year={2004},
  publisher={Springer}
}


@misc{iso26262,
    author={International Standards Organization},
    title={{ISO} 26262-1:2011, Road vehicles --- Functional safety},
    howpublished={\href{https://www.iso.org/standard/43464.html}{https://www.iso.org/standard/43464.html} (visited 2022-04-20)},
    year={2018},
}

@article{monmasson2011fpgas,
  title={FPGAs in industrial control applications},
  author={Monmasson, Eric and Idkhajine, Lahoucine and Cirstea, Marcian N and Bahri, Imene and Tisan, Alin and Naouar, Mohamed Wissem},
  journal={IEEE Transactions on Industrial informatics},
  volume={7},
  number={2},
  pages={224--243},
  year={2011},
  publisher={IEEE}
}


@article{rudin1992nonlinear,
  title={Nonlinear total variation based noise removal algorithms},
  author={Rudin, Leonid I and Osher, Stanley and Fatemi, Emad},
  journal={Physica D: nonlinear phenomena},
  volume={60},
  number={1-4},
  pages={259--268},
  year={1992},
  publisher={Elsevier}
}


@article{fukuda1992theory,
  title={Theory and applications of neural networks for industrial control systems},
  author={Fukuda, Toshio and Shibata, Takanori},
  journal={IEEE Transactions on industrial electronics},
  volume={39},
  number={6},
  pages={472--489},
  year={1992},
  publisher={IEEE}
}

@article{reverse_sigmoid,
  author    = {Taesung Lee and
               Benjamin Edwards and
               Ian M. Molloy and
               Dong Su},
  title     = {Defending Against Model Stealing Attacks Using Deceptive Perturbations},
  journal   = {CoRR},
  volume    = {abs/1806.00054},
  year      = {2018},
  url       = {http://arxiv.org/abs/1806.00054},
  eprinttype = {arXiv},
  eprint    = {1806.00054},
  timestamp = {Wed, 02 Jun 2021 09:13:29 +0200},
  biburl    = {https://dblp.org/rec/journals/corr/abs-1806-00054.bib},
  bibsource = {dblp computer science bibliography, https://dblp.org}
}

@article{sahiner2019deep,
  title={Deep learning in medical imaging and radiation therapy},
  author={Sahiner, Berkman and Pezeshk, Aria and Hadjiiski, Lubomir M and Wang, Xiaosong and Drukker, Karen and Cha, Kenny H and Summers, Ronald M and Giger, Maryellen L},
  journal={Medical physics},
  volume={46},
  number={1},
  pages={e1--e36},
  year={2019},
  publisher={Wiley Online Library}
}

@book{pearson2005mining,
  title={Mining imperfect data: Dealing with contamination and incomplete records},
  author={Pearson, Ronald K},
  year={2005},
  publisher={SIAM}
}

@ARTICLE{ching2017opportunities,
    author = {Ching, Travers
              and Himmelstein, Daniel~S.
              and Beaulieu-Jones, Brett~K.
              and Kalinin, Alexandr~A.
              and Do, Brian~T.
              and Way, Gregory~P.
              and Ferrero, Enrico
              and Agapow, Paul-Michael
              and Zietz, Michael
              and Hoffman, Michael~M.
              and Xie, Wei
              and Rosen, Gail~L.
              and Lengerich, Benjamin~J.
              and Israeli, Johnny
              and Lanchantin, Jack
              and Woloszynek, Stephen
              and Carpenter, Anne~E.
              and Shrikumar, Avanti
              and Xu, Jinbo
              and Cofer, Evan~M.
              and Lavender, Christopher~A.
              and Turaga, Srinivas~C.
              and Alexandari, Amr~M.
              and Lu, Zhiyong
              and Harris, David~J.
              and DeCaprio, Dave
              and Qi, Yanjun
              and Kundaje, Anshul
              and Peng, Yifan
              and Wiley, Laura~K.
              and Segler, Marwin~H.~S.
              and Boca, Simina~M.
              and Swamidass, S.~ Joshua
              and Huang, Austin
              and Gitter, Anthony
              and Greene, Casey~S.},
    title = {Opportunities and obstacles for deep learning in biology and medicine},
    journal = {Journal of the Royal Society Interface},
    year = {2017},
    volume = {15},
    number = {141},
}


@inproceedings{bernal2017safety++,
  title={Safety++ designing {IoT} and wearable systems for industrial safety through a user centered design approach},
  author={Bernal, Guillermo and Colombo, Sara and Al Ai Baky, Mohammed and Casalegno, Federico},
  booktitle={Proceedings of the 10th International Conference on Pervasive Technologies Related to Assistive Environments},
  pages={163--170},
  year={2017}
}


@article{pakdemirli2019artificial,
  title={Artificial intelligence in radiology: friend or foe? Where are we now and where are we heading?},
  author={Pakdemirli, Emre},
  journal={Acta radiologica open},
  volume={8},
  number={2},
  year={2019},
  publisher={SAGE Publications Sage UK: London, England}
}

@misc{nhtsa,
    title={Critical Reasons for Crashes Investigated in the National Motor Vehicle Crash Causation Survey},
    howpublished={\href{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115}{https://crashstats.nhtsa.dot.gov/Api/Public/ViewPublication/812115} (visited 2022-04-20)},
    author={National Highway Transportation Safety Administration's (NHTSA) National Center for Statistics and Analysis},
    publisher={US Department of Transportation},
    year={2015},
}

@article{icoh, title={GLOBAL ESTIMATES OF OCCUPATIONAL  ACCIDENTS AND WORK-RELATED ILLNESSES 2017}, url={http://www.icohweb.org/site/images/news/pdf/Report\%20Global\%20Estimates\%20of\%20Occupational\%20Accidents\%20and\%20Work-related\%20Illnesses\%202017\%20rev1.pdf}, journal={International Commission on Occupational Health`}, publisher={ICOH}, author={ICOH}, year={2017}} 

@misc{OECD,
    title={{OECD} statistics},
    howpublished={\href{https://stats.oecd.org}{https://stats.oecd.org/} (visited 2022-04-20)},
    journal={{OECD} Statistics},
    publisher={International Transport Forum},
    author={The Organisation for Economic Co-operation and Development},
    year={2020},
}

@article{makary2016medical,
  title={Medical error---the third leading cause of death in the {US}},
  author={Makary, Martin A and Daniel, Michael},
  journal={{BMJ}},
  volume={353},
  year={2016},
  publisher={British Medical Journal Publishing Group}
}


@article{imageenhance,
  title={Relationship between entropy and SNR changes in image enhancement},
  author={Krbcova, Zuzana and Kukal, Jaromir},
  journal={EURASIP Journal on Image and Video Processing},
  volume={2017},
  number={1},
  pages={1--8},
  year={2017},
  publisher={Springer}
}
@article{chung1988labelings,
  title={Labelings of graphs},
  author={Chung, Fan RK},
  journal={Selected topics in graph theory},
  volume={3},
  pages={151--168},
  year={1988}
}

@inproceedings{hamming,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}

@inproceedings{testautomation,
  title={Test automation for safety-critical systems: Industrial application and future developments},
  author={Peleska, Jan},
  booktitle={International Symposium of Formal Methods Europe},
  pages={39--59},
  year={1996},
  organization={Springer}
}

@book{aviation_compliance,
  title={Developing safety-critical software: a practical guide for aviation software and DO-178C compliance},
  author={Rierson, Leanna},
  year={2017},
  publisher={CRC Press}
}

@article{safetyframework,
  title={A framework for safety automation of safety-critical systems operations},
  author={Acharyulu, PV Srinivas and Seetharamaiah, P},
  journal={Safety Science},
  volume={77},
  pages={133--142},
  year={2015},
  publisher={Elsevier}
}

@article{papernot_distillation_2016,
	title = {Distillation as a {Defense} to {Adversarial} {Perturbations} against {Deep} {Neural} {Networks}},
	url = {http://arxiv.org/abs/1511.04508},
	abstract = {Deep learning algorithms have been shown to perform extremely well on many classical machine learning problems. However, recent studies have shown that deep learning, like other machine learning techniques, is vulnerable to adversarial samples: inputs crafted to force a deep neural network (DNN) to provide adversary-selected outputs. Such attacks can seriously undermine the security of the system supported by the DNN, sometimes with devastating consequences. For example, autonomous vehicles can be crashed, illicit or illegal content can bypass content ﬁlters, or biometric authentication systems can be manipulated to allow improper access. In this work, we introduce a defensive mechanism called defensive distillation to reduce the effectiveness of adversarial samples on DNNs. We analytically investigate the generalizability and robustness properties granted by the use of defensive distillation when training DNNs. We also empirically study the effectiveness of our defense mechanisms on two DNNs placed in adversarial settings. The study shows that defensive distillation can reduce effectiveness of sample creation from 95\% to less than 0.5\% on a studied DNN. Such dramatic gains can be explained by the fact that distillation leads gradients used in adversarial sample creation to be reduced by a factor of 1030. We also ﬁnd that distillation increases the average minimum number of features that need to be modiﬁed to create adversarial samples by about 800\% on one of the DNNs we tested.},
	language = {en},
	urldate = {2020-11-03},
	journal = {arXiv:1511.04508 [cs, stat]},
	author = {Papernot, Nicolas and McDaniel, Patrick and Wu, Xi and Jha, Somesh and Swami, Ananthram},
	month = mar,
	year = {2016},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning, Computer Science - Neural and Evolutionary Computing},
	file = {Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:C\:\\Users\\charlie\\Zotero\\storage\\YTLSC2LI\\Papernot et al. - 2016 - Distillation as a Defense to Adversarial Perturbat.pdf:application/pdf}
}

@article{vadera_assessing_2020,
	title = {Assessing the {Adversarial} {Robustness} of {Monte} {Carlo} and {Distillation} {Methods} for {Deep} {Bayesian} {Neural} {Network} {Classification}},
	url = {http://arxiv.org/abs/2002.02842},
	abstract = {In this paper, we consider the problem of assessing the adversarial robustness of deep neural network models under both Markov chain Monte Carlo (MCMC) and Bayesian Dark Knowledge (BDK) inference approximations. We characterize the robustness of each method to two types of adversarial attacks: the fast gradient sign method (FGSM) and projected gradient descent (PGD). We show that full MCMC-based inference has excellent robustness, signiﬁcantly outperforming standard point estimation-based learning. On the other hand, BDK provides marginal improvements. As an additional contribution, we present a storage-efﬁcient approach to computing adversarial examples for large Monte Carlo ensembles using both the FGSM and PGD attacks.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:2002.02842 [cs, stat]},
	author = {Vadera, Meet P. and Shukla, Satya Narayan and Jalaian, Brian and Marlin, Benjamin M.},
	month = feb,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {2002.02842[1].pdf:C\:\\Users\\charlie\\AppData\\Local\\Microsoft\\Windows\\INetCache\\IE\\M03SCMDN\\2002.02842[1].pdf:application/pdf}
}

@article{yu_bayesian_2016,
	title = {A {Bayesian} {Ensemble} for {Unsupervised} {Anomaly} {Detection}},
	url = {http://arxiv.org/abs/1610.07677},
	abstract = {Methods for unsupervised anomaly detection suﬀer from the fact that the data is unlabeled, making it diﬃcult to assess the optimality of detection algorithms. Ensemble learning has shown exceptional results in classiﬁcation and clustering problems, but has not seen as much research in the context of outlier detection. Existing methods focus on combining output scores of individual detectors, but this leads to outputs that are not easily interpretable. In this paper, we introduce a theoretical foundation for combining individual detectors with Bayesian classiﬁer combination. Not only are posterior distributions easily interpreted as the probability distribution of anomalies, but bias, variance, and individual error rates of detectors are all easily obtained. Performance on real-world datasets shows high accuracy across varied types of time series data.},
	language = {en},
	urldate = {2020-07-20},
	journal = {arXiv:1610.07677 [cs, stat]},
	author = {Yu, Edward and Parekh, Parth},
	month = oct,
	year = {2016},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning}
}

@article{simon-gabriel_first-order_2019,
	title = {First-order {Adversarial} {Vulnerability} of {Neural} {Networks} and {Input} {Dimension}},
	url = {http://arxiv.org/abs/1802.01421},
	abstract = {Over the past few years, neural networks were proven vulnerable to adversarial images: targeted but imperceptible image perturbations lead to drastically different predictions. We show that adversarial vulnerability increases with the gradients of the training objective when viewed as a function of the inputs. Surprisingly, vulnerability does not depend on network topology: for many standard network architectures, we prove that at initialization, the 1-norm of these gradients grows as the square root of the input dimension, leaving the networks increasingly vulnerable with growing image size. We empirically show that this dimension dependence persists after either usual or robust training, but gets attenuated with higher regularization.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1802.01421 [cs, stat]},
	author = {Simon-Gabriel, Carl-Johann and Ollivier, Yann and Bottou, Léon and Schölkopf, Bernhard and Lopez-Paz, David},
	month = jun,
	year = {2019},
	keywords = {68T45, Computer Science - Computer Vision and Pattern Recognition, Computer Science - Machine Learning, I.2.6, Statistics - Machine Learning},
	file = {Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:C\:\\Users\\charlie\\Zotero\\storage\\HNKCWFZ8\\Simon-Gabriel et al. - 2019 - First-order Adversarial Vulnerability of Neural Ne.pdf:application/pdf}
}

@article{inkawhich_adversarial_2018,
	title = {Adversarial {Attacks} for {Optical} {Flow}-{Based} {Action} {Recognition} {Classifiers}},
	url = {http://arxiv.org/abs/1811.11875},
	abstract = {The success of deep learning research has catapulted deep models into production systems that our society is becoming increasingly dependent on, especially in the image and video domains. However, recent work has shown that these largely uninterpretable models exhibit glaring security vulnerabilities in the presence of an adversary. In this work, we develop a powerful untargeted adversarial attack for action recognition systems in both white-box and black-box settings. Action recognition models differ from image-classiﬁcation models in that their inputs contain a temporal dimension, which we explicitly target in the attack. Drawing inspiration from image classiﬁer attacks, we create new attacks which achieve state-of-the-art success rates on a two-stream classiﬁer trained on the UCF-101 dataset. We ﬁnd that our attacks can signiﬁcantly degrade a model’s performance with sparsely and imperceptibly perturbed examples. We also demonstrate the transferability of our attacks to black-box action recognition systems.},
	language = {en},
	urldate = {2020-09-17},
	journal = {arXiv:1811.11875 [cs]},
	author = {Inkawhich, Nathan and Inkawhich, Matthew and Chen, Yiran and Li, Hai},
	month = nov,
	year = {2018},
	keywords = {Computer Science - Computer Vision and Pattern Recognition},
	file = {Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:C\:\\Users\\charlie\\Zotero\\storage\\KAR3FYXU\\Inkawhich et al. - 2018 - Adversarial Attacks for Optical Flow-Based Action .pdf:application/pdf}
}

@article{uesato_adversarial_2018,
	title = {Adversarial {Risk} and the {Dangers} of {Evaluating} {Against} {Weak} {Attacks}},
	url = {http://arxiv.org/abs/1802.05666},
	abstract = {This paper investigates recently proposed approaches for defending against adversarial examples and evaluating adversarial robustness. We motivate adversarial risk as an objective for achieving models robust to worst-case inputs. We then frame commonly used attacks and evaluation metrics as deﬁning a tractable surrogate objective to the true adversarial risk. This suggests that models may optimize this surrogate rather than the true adversarial risk. We formalize this notion as obscurity to an adversary, and develop tools and heuristics for identifying obscured models and designing transparent models. We demonstrate that this is a signiﬁcant problem in practice by repurposing gradient-free optimization techniques into adversarial attacks, which we use to decrease the accuracy of several recently proposed defenses to near zero. Our hope is that our formulations and results will help researchers to develop more powerful defenses.},
	language = {en},
	urldate = {2020-09-28},
	journal = {arXiv:1802.05666 [cs, stat]},
	author = {Uesato, Jonathan and O'Donoghue, Brendan and Oord, Aaron van den and Kohli, Pushmeet},
	month = jun,
	year = {2018},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:C\:\\Users\\charlie\\Zotero\\storage\\V4RNGWRE\\Uesato et al. - 2018 - Adversarial Risk and the Dangers of Evaluating Aga.pdf:application/pdf}
}


@article{christmann_robustness_nodate,
	title = {On {Robustness} {Properties} of {Convex} {Risk} {Minimization} {Methods} for {Pattern} {Recognition}},
	abstract = {The paper brings together methods from two disciplines: machine learning theory and robust statistics. We argue that robustness is an important aspect and we show that many existing machine learning methods based on the convex risk minimization principle have − besides other good properties − also the advantage of being robust. Robustness properties of machine learning methods based on convex risk minimization are investigated for the problem of pattern recognition. Assumptions are given for the existence of the inﬂuence function of the classiﬁers and for bounds on the inﬂuence function. Kernel logistic regression, support vector machines, least squares and the AdaBoost loss function are treated as special cases. Some results on the robustness of such methods are also obtained for the sensitivity curve and the maxbias, which are two other robustness criteria. A sensitivity analysis of the support vector machine is given.},
	language = {en},
	author = {Christmann, Andreas and Steinwart, Ingo},
	pages = {28},
	file = {christmann04a.pdf:C\:\\Users\\charlie\\Zotero\\storage\\A3EZEX5B\\christmann04a.pdf:application/pdf}
}

@article{min_curious_2020,
	title = {The {Curious} {Case} of {Adversarially} {Robust} {Models}: {More} {Data} {Can} {Help}, {Double} {Descend}, or {Hurt} {Generalization}},
	shorttitle = {The {Curious} {Case} of {Adversarially} {Robust} {Models}},
	url = {http://arxiv.org/abs/2002.11080},
	abstract = {Adversarial training has shown its ability in producing models that are robust to perturbations on the input data, but usually at the expense of decrease in the standard accuracy. To mitigate this issue, it is commonly believed that more training data will eventually help such adversarially robust models generalize better on the benign/unperturbed test data. In this paper, however, we challenge this conventional belief and show that more training data can hurt the generalization of adversarially robust models in the classiﬁcation problems. We ﬁrst investigate the Gaussian mixture classiﬁcation with a linear loss and identify three regimes based on the strength of the adversary. In the weak adversary regime, more data improves the generalization of adversarially robust models. In the medium adversary regime, with more training data, the generalization loss exhibits a double descent curve, which implies the existence of an intermediate stage where more training data hurts the generalization. In the strong adversary regime, more data almost immediately causes the generalization error to increase. Then we move to the analysis of a two-dimensional classiﬁcation problem with a 0-1 loss. We prove that more data always hurts the generalization performance of adversarially trained models with large perturbations. To complement our theoretical results, we conduct empirical studies on Gaussian mixture classiﬁcation, support vector machines (SVMs), and linear regression.},
	language = {en},
	urldate = {2020-10-02},
	journal = {arXiv:2002.11080 [cs, stat]},
	author = {Min, Yifei and Chen, Lin and Karbasi, Amin},
	month = jun,
	year = {2020},
	keywords = {Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:C\:\\Users\\charlie\\Zotero\\storage\\AZLGP9IT\\Min et al. - 2020 - The Curious Case of Adversarially Robust Models M.pdf:application/pdf}
}



@ARTICLE{distributed_attacks,

  author={Aljuhani, Ahamed},

  journal={IEEE Access}, 

  title={Machine Learning Approaches for Combating Distributed Denial of Service Attacks in Modern Networking Environments}, 

  year={2021},

  volume={9},

  number={},

  pages={42236-42264},

  doi={10.1109/ACCESS.2021.3062909}}



@INPROCEEDINGS{intermittent,

  author={Park, Jeman and Nyang, DaeHun and Mohaisen, Aziz},

  booktitle={2018 16th Annual Conference on Privacy, Security and Trust (PST)}, 

  title={Timing is Almost Everything: Realistic Evaluation of the Very Short Intermittent DDoS Attacks}, 

  year={2018},

  volume={},

  number={},

  pages={1-10},

  doi={10.1109/PST.2018.8514210}}





@article{ziai_active_2019,
	title = {Active {Learning} for {Network} {Intrusion} {Detection}},
	url = {http://arxiv.org/abs/1904.01555},
	abstract = {Network operators are generally aware of common attack vectors that they defend against. For most networks the vast majority of traffic is legitimate. However new attack vectors are continually designed and attempted by bad actors which bypass detection and go unnoticed due to low volume. One strategy for finding such activity is to look for anomalous behavior. Investigating anomalous behavior requires significant time and resources. Collecting a large number of labeled examples for training supervised models is both prohibitively expensive and subject to obsoletion as new attacks surface. A purely unsupervised methodology is ideal; however, research has shown that even a very small number of labeled examples can significantly improve the quality of anomaly detection. A methodology that minimizes the number of required labels while maximizing the quality of detection is desirable. False positives in this context result in wasted effort or blockage of legitimate traffic and false negatives translate to undetected attacks. We propose a general active learning framework and experiment with different choices of learners and sampling strategies.},
	urldate = {2020-11-02},
	journal = {arXiv:1904.01555 [cs, stat]},
	author = {Ziai, Amir},
	month = apr,
	year = {2019},
	keywords = {Computer Science - Cryptography and Security, Computer Science - Machine Learning, Statistics - Machine Learning},
	file = {arXiv Fulltext PDF:C\:\\Users\\charlie\\Zotero\\storage\\I6PUG64G\\Ziai - 2019 - Active Learning for Network Intrusion Detection.pdf:application/pdf;arXiv.org Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\MPS8FFA3\\1904.html:text/html}
}

@inproceedings{zhang_cross-dataset_2019,
	address = {USA},
	series = {{USENIX} {ATC} '19},
	title = {Cross-dataset time series anomaly detection for cloud systems},
	isbn = {978-1-939133-03-8},
	abstract = {In recent years, software applications are increasingly deployed as online services on cloud computing platforms. It is important to detect anomalies in cloud systems in order to maintain high service availability. However, given the velocity, volume, and diversified nature of cloud monitoring data, it is difficult to obtain sufficient labelled data to build an accurate anomaly detection model. In this paper, we propose cross-dataset anomaly detection: detect anomalies in a new unlabelled dataset (the target) by training an anomaly detection model on existing labelled datasets (the source). Our approach, called ATAD (Active Transfer Anomaly Detection), integrates both transfer learning and active learning techniques. Transfer learning is applied to transfer knowledge from the source dataset to the target dataset, and active learning is applied to determine informative labels of a small part of samples from unlabelled datasets. Through experiments, we show that ATAD is effective in cross-dataset time series anomaly detection. Furthermore, we only need to label about 1\%-5\% of unlabelled data and can still achieve significant performance improvement.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	publisher = {USENIX Association},
	author = {Zhang, Xu and Lin, Qingwei and Xu, Yong and Qin, Si and Zhang, Hongyu and Qiao, Bo and Dang, Yingnong and Yang, Xinsheng and Cheng, Qian and Chintalapati, Murali and Wu, Youjiang and Hsieh, Ken and Sui, Kaixin and Meng, Xin and Xu, Yaohai and Zhang, Wenchi and Shen, Furao and Zhang, Dongmei},
	month = jul,
	year = {2019},
	pages = {1063--1076}
}

@inproceedings{jang_objective_2017,
	address = {Orlando FL USA},
	title = {Objective {Metrics} and {Gradient} {Descent} {Algorithms} for {Adversarial} {Examples} in {Machine} {Learning}},
	isbn = {978-1-4503-5345-8},
	url = {https://dl.acm.org/doi/10.1145/3134600.3134635},
	doi = {10.1145/3134600.3134635},
	abstract = {Fueled by massive amounts of data, models produced by machinelearning (ML) algorithms are being used in diverse domains where security is a concern, such as, automotive systems, finance, health-care, computer vision, speech recognition, naturallanguage processing, and malware detection. Of particular concern is use of ML in cyberphysical systems, such as driverless cars and aviation, where the presence of an adversary can cause serious consequences. In this paper we focus on attacks caused by adversarial samples, which are inputs crafted by adding small, often imperceptible, perturbations to force a ML model to misclassify. We present a simple gradient-descent based algorithm for finding adversarial samples, which performs well in comparison to existing algorithms. The second issue that this paper tackles is that of metrics. We present a novel metric based on few computer-vision algorithms for measuring the quality of adversarial samples.},
	language = {en},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the 33rd {Annual} {Computer} {Security} {Applications} {Conference}},
	publisher = {ACM},
	author = {Jang, Uyeong and Wu, Xi and Jha, Somesh},
	month = dec,
	year = {2017},
	pages = {262--277},
	file = {Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:C\:\\Users\\charlie\\Zotero\\storage\\AWDDBQE7\\Jang et al. - 2017 - Objective Metrics and Gradient Descent Algorithms .pdf:application/pdf}
}

@inproceedings{thill_online_2017,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8}
}

@article{ko_loss-driven_2019,
	title = {Loss-{Driven} {Adversarial} {Ensemble} {Deep} {Learning} for {On}-{Line} {Time} {Series} {Analysis}},
	volume = {11},
	copyright = {http://creativecommons.org/licenses/by/3.0/},
	url = {https://www.mdpi.com/2071-1050/11/12/3489},
	doi = {10.3390/su11123489},
	abstract = {Developing a robust and sustainable system is an important problem in which deep learning models are used in real-world applications. Ensemble methods combine diverse models to improve performance and achieve robustness. The analysis of time series data requires dealing with continuously incoming instances; however, most ensemble models suffer when adapting to a change in data distribution. Therefore, we propose an on-line ensemble deep learning algorithm that aggregates deep learning models and adjusts the ensemble weight based on loss value in this study. We theoretically demonstrate that the ensemble weight converges to the limiting distribution, and, thus, minimizes the average total loss from a new regret measure based on adversarial assumption. We also present an overall framework that can be applied to analyze time series. In the experiments, we focused on the on-line phase, in which the ensemble models predict the binary class for the simulated data and the financial and non-financial real data. The proposed method outperformed other ensemble approaches. Moreover, our method was not only robust to the intentional attacks but also sustainable in data distribution changes. In the future, our algorithm can be extended to regression and multiclass classification problems.},
	language = {en},
	number = {12},
	urldate = {2020-11-02},
	journal = {Sustainability},
	author = {Ko, Hyungjin and Lee, Jaewook and Byun, Junyoung and Son, Bumho and Park, Saerom},
	month = jan,
	year = {2019},
	note = {Number: 12
Publisher: Multidisciplinary Digital Publishing Institute},
	keywords = {adaptive learning, ensemble deep learning, on-line learning, time series analysis},
	pages = {3489},
	file = {Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\2MWPWFGD\\3489.html:text/html;Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\GWZ7CFVU\\Ko et al. - 2019 - Loss-Driven Adversarial Ensemble Deep Learning for.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\ACQARLLN\\3489.html:text/html}
}

@article{tiwari_binary_2019,
	title = {Binary {Classifier} {Inspired} by {Quantum} {Theory}},
	volume = {33},
	copyright = {Copyright (c) 2019 Association for the Advancement of Artificial Intelligence},
	issn = {2374-3468},
	url = {https://www.aaai.org/ojs/index.php/AAAI/article/view/5162},
	doi = {10.1609/aaai.v33i01.330110051},
	abstract = {Machine Learning (ML) helps us to recognize patterns from raw data. ML is used in numerous domains i.e. biomedical, agricultural, food technology, etc. Despite recent technological advancements, there is still room for substantial improvement in prediction. Current ML models are based on classical theories of probability and statistics, which can now be replaced by Quantum Theory (QT) with the aim of improving the effectiveness of ML. In this paper, we propose the Binary Classifier Inspired by Quantum Theory (BCIQT) model, which outperforms the state of the art classification in terms of recall for every category.},
	language = {en},
	number = {01},
	urldate = {2020-11-02},
	journal = {Proceedings of the AAAI Conference on Artificial Intelligence},
	author = {Tiwari, Prayag and Melucci, Massimo},
	month = jul,
	year = {2019},
	note = {Number: 01},
	pages = {10051--10052},
	file = {Full Text PDF:C\:\\Users\\charlie\\Zotero\\storage\\U2PBJDV3\\Tiwari and Melucci - 2019 - Binary Classifier Inspired by Quantum Theory.pdf:application/pdf;Snapshot:C\:\\Users\\charlie\\Zotero\\storage\\K7AHN2DJ\\5162.html:text/html}
}

@inproceedings{thill_online_2017-1,
	title = {Online anomaly detection on the webscope {S5} dataset: {A} comparative study},
	shorttitle = {Online anomaly detection on the webscope {S5} dataset},
	doi = {10.1109/EAIS.2017.7954844},
	abstract = {An unresolved challenge for all kind of temporal data is the reliable anomaly detection, especially when adaptability is required in the case of non-stationary time series or when the nature of future anomalies is unknown or only vaguely defined. Most of the current anomaly detection algorithms follow the general idea to classify an anomaly as a significant deviation from the prediction. In this paper we present a comparative study where several online anomaly detection algorithms are compared on the large Yahoo Webscope S5 anomaly benchmark. We show that a relatively Simple Online Regression Anomaly Detector (SORAD) is quite successful compared to other anomaly detectors. We discuss the importance of several adaptive and online elements of the algorithm and their influence on the overall anomaly detection accuracy.},
	booktitle = {2017 {Evolving} and {Adaptive} {Intelligent} {Systems} ({EAIS})},
	author = {Thill, M. and Konen, W. and Bäck, T.},
	month = may,
	year = {2017},
	note = {ISSN: 2473-4691},
	keywords = {Benchmark testing, data handling, Detection algorithms, Internet, nonstationary time series, online anomaly detection, Prediction algorithms, regression analysis, simple online regression anomaly detector, SORAD, temporal data, time series, Time series analysis, Training, Training data, Transient analysis, Yahoo Webscope S5 anomaly benchmark},
	pages = {1--8},
	file = {IEEE Xplore Abstract Record:C\:\\Users\\charlie\\Zotero\\storage\\VBVMH5UI\\7954844.html:text/html}
}

@misc{noauthor_cross-dataset_nodate,
	title = {Cross-dataset time series anomaly detection for cloud systems {\textbar} {Proceedings} of the 2019 {USENIX} {Conference} on {Usenix} {Annual} {Technical} {Conference}},
	url = {https://dl.acm.org/doi/10.5555/3358807.3358898},
	urldate = {2020-11-02},
	file = {Cross-dataset time series anomaly detection for cloud systems | Proceedings of the 2019 USENIX Conference on Usenix Annual Technical Conference:C\:\\Users\\charlie\\Zotero\\storage\\X76DR7WP\\3358807.html:text/html}
}




@inproceedings{lowd_adversarial_2005,
	address = {New York, NY, USA},
	series = {{KDD} '05},
	title = {Adversarial learning},
	isbn = {978-1-59593-135-1},
	url = {https://doi.org/10.1145/1081870.1081950},
	doi = {10.1145/1081870.1081950},
	abstract = {Many classification tasks, such as spam filtering, intrusion detection, and terrorism detection, are complicated by an adversary who wishes to avoid detection. Previous work on adversarial classification has made the unrealistic assumption that the attacker has perfect knowledge of the classifier [2]. In this paper, we introduce the adversarial classifier reverse engineering (ACRE) learning problem, the task of learning sufficient information about a classifier to construct adversarial attacks. We present efficient algorithms for reverse engineering linear classifiers with either continuous or Boolean features and demonstrate their effectiveness using real data from the domain of spam filtering.},
	urldate = {2020-11-02},
	booktitle = {Proceedings of the eleventh {ACM} {SIGKDD} international conference on {Knowledge} discovery in data mining},
	publisher = {Association for Computing Machinery},
	author = {Lowd, Daniel and Meek, Christopher},
	month = aug,
	year = {2005},
	keywords = {adversarial classification, linear classifiers, spam},
	pages = {641--647}
}

@article{tramer_stealing_nodate,
	title = {Stealing {Machine} {Learning} {Models} via {Prediction} {APIs}},
	abstract = {Machine learning (ML) models may be deemed confidential due to their sensitive training data, commercial value, or use in security applications. Increasingly often, confidential ML models are being deployed with publicly accessible query interfaces. ML-as-a-service (“predictive analytics”) systems are an example: Some allow users to train models on potentially sensitive data and charge others for access on a pay-per-query basis.},
	language = {en},
	author = {Tramer, Florian and Zhang, Fan and Juels, Ari and Reiter, Michael K and Ristenpart, Thomas},
	pages = {19},
	file = {Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:C\:\\Users\\charlie\\Zotero\\storage\\Q4ZJNER6\\Tramer et al. - Stealing Machine Learning Models via Prediction AP.pdf:application/pdf}
}

@inproceedings{ruan2019global,
  title={Global robustness evaluation of deep neural networks with provable guarantees for the hamming distance},
  author={Ruan, Wenjie and Wu, Min and Sun, Youcheng and Huang, Xiaowei and Kroening, Daniel and Kwiatkowska, Marta},
  year={2019},
  organization={IJCAI}
}